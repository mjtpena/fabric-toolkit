{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32365d65",
   "metadata": {},
   "source": [
    "# Microsoft Fabric Lakehouse Restore Notebook\n",
    "\n",
    "This notebook provides restore capabilities for Microsoft Fabric Lakehouse backups created with the Fabric_Lakehouse_Backup.ipynb notebook.\n",
    "\n",
    "## Features:\n",
    "- Restore all tables or specific tables from backup\n",
    "- Cross-workspace restore capabilities\n",
    "- Delta transaction log restoration\n",
    "- Backup manifest validation\n",
    "- Progress tracking and detailed logging\n",
    "- Support for all backup types (Storage Account, Lakehouse, ADLS)\n",
    "\n",
    "## Use Cases:\n",
    "- Disaster recovery\n",
    "- Point-in-time restore\n",
    "- Environment synchronization (dev/test/prod)\n",
    "- Data migration between workspaces\n",
    "\n",
    "**Author:** Generated for Microsoft Fabric  \n",
    "**Version:** 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac257d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: Restore Configuration Parameters\n",
    "# ============================================================================\n",
    "\n",
    "# Configure these parameters before running the restore operation\n",
    "dbutils.widgets.text(\"restore_source_path\", \"\", \"Full Path to Backup Location\")\n",
    "dbutils.widgets.text(\"restore_target_lakehouse\", \"\", \"Target Lakehouse Name\")\n",
    "dbutils.widgets.text(\"restore_target_workspace_id\", \"\", \"Target Workspace ID (leave empty for current workspace)\")\n",
    "dbutils.widgets.dropdown(\"restore_specific_tables\", \"False\", [\"True\", \"False\"], \"Restore Specific Tables Only\")\n",
    "dbutils.widgets.text(\"tables_to_restore\", \"\", \"Tables to Restore (comma-separated, if specific)\")\n",
    "dbutils.widgets.dropdown(\"overwrite_existing\", \"True\", [\"True\", \"False\"], \"Overwrite Existing Tables\")\n",
    "dbutils.widgets.dropdown(\"restore_delta_logs\", \"True\", [\"True\", \"False\"], \"Restore Delta Transaction Logs\")\n",
    "dbutils.widgets.dropdown(\"validate_before_restore\", \"True\", [\"True\", \"False\"], \"Validate Backup Before Restore\")\n",
    "dbutils.widgets.dropdown(\"verify_after_restore\", \"True\", [\"True\", \"False\"], \"Verify Restore After Completion\")\n",
    "dbutils.widgets.dropdown(\"enable_detailed_logging\", \"True\", [\"True\", \"False\"], \"Enable Detailed Logging\")\n",
    "\n",
    "print(\"Restore parameters configured. Ready to begin restore operation.\")\n",
    "print(\"Available backup types: Storage Account, Lakehouse, ADLS\")\n",
    "print(\"Support for cross-workspace operations enabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07878a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Import Required Libraries\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import uuid\n",
    "from pyspark.sql.functions import lit, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n",
    "from delta.tables import DeltaTable\n",
    "import time\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Restore process initiated at: {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a1e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Restore Helper Functions\n",
    "# ============================================================================\n",
    "\n",
    "def log_message(message, level=\"INFO\"):\n",
    "    \"\"\"Log a message with timestamp and level\"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{timestamp}] [{level}] {message}\")\n",
    "\n",
    "def validate_restore_parameters():\n",
    "    \"\"\"Validate that required restore parameters are provided\"\"\"\n",
    "    restore_source_path = dbutils.widgets.get(\"restore_source_path\")\n",
    "    restore_target_lakehouse = dbutils.widgets.get(\"restore_target_lakehouse\")\n",
    "    \n",
    "    if not restore_source_path:\n",
    "        raise ValueError(\"Restore source path is required\")\n",
    "    \n",
    "    if not restore_target_lakehouse:\n",
    "        raise ValueError(\"Target lakehouse name is required\")\n",
    "    \n",
    "    # Check if source path exists\n",
    "    try:\n",
    "        dbutils.fs.ls(restore_source_path)\n",
    "        log_message(f\"Backup source validated: {restore_source_path}\", \"INFO\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Backup source path does not exist or is not accessible: {restore_source_path}\")\n",
    "\n",
    "def get_target_path():\n",
    "    \"\"\"Construct the target path for restore operation\"\"\"\n",
    "    target_lakehouse = dbutils.widgets.get(\"restore_target_lakehouse\")\n",
    "    target_workspace_id = dbutils.widgets.get(\"restore_target_workspace_id\")\n",
    "    \n",
    "    if target_workspace_id and target_workspace_id.strip():\n",
    "        # External workspace lakehouse\n",
    "        return f\"abfss://{target_workspace_id}@onelake.dfs.fabric.microsoft.com/{target_lakehouse}.Lakehouse/Tables\"\n",
    "    else:\n",
    "        # Current workspace lakehouse\n",
    "        return \"Tables\"\n",
    "\n",
    "def read_backup_manifest(restore_source_path):\n",
    "    \"\"\"Read and validate the backup manifest\"\"\"\n",
    "    try:\n",
    "        manifest_path = f\"{restore_source_path}/_manifest\"\n",
    "        manifest_df = spark.read.format(\"json\").load(manifest_path)\n",
    "        manifest_data = manifest_df.collect()[0].asDict()\n",
    "        \n",
    "        log_message(\"Backup manifest loaded successfully\", \"INFO\")\n",
    "        log_message(f\"Backup ID: {manifest_data.get('backup_id', 'Unknown')}\", \"INFO\")\n",
    "        log_message(f\"Backup timestamp: {manifest_data.get('backup_timestamp', 'Unknown')}\", \"INFO\")\n",
    "        log_message(f\"Source lakehouse: {manifest_data.get('source_lakehouse_name', 'Unknown')}\", \"INFO\")\n",
    "        log_message(f\"Tables in backup: {len(manifest_data.get('tables', []))}\", \"INFO\")\n",
    "        \n",
    "        return manifest_data\n",
    "    except Exception as e:\n",
    "        log_message(f\"Warning: Could not read backup manifest: {str(e)}\", \"WARNING\")\n",
    "        return None\n",
    "\n",
    "def get_tables_to_restore(restore_source_path, manifest_data=None):\n",
    "    \"\"\"Get list of tables to restore based on parameters\"\"\"\n",
    "    restore_specific = dbutils.widgets.get(\"restore_specific_tables\") == \"True\"\n",
    "    tables_str = dbutils.widgets.get(\"tables_to_restore\")\n",
    "    \n",
    "    if restore_specific and tables_str.strip():\n",
    "        # User specified specific tables\n",
    "        tables_to_restore = [t.strip() for t in tables_str.split(\",\")]\n",
    "        log_message(f\"Restoring specific tables: {', '.join(tables_to_restore)}\", \"INFO\")\n",
    "        return tables_to_restore\n",
    "    else:\n",
    "        # Get all tables from backup\n",
    "        if manifest_data and 'tables' in manifest_data:\n",
    "            # Use manifest if available\n",
    "            tables_to_restore = manifest_data['tables']\n",
    "            log_message(f\"Using manifest: Found {len(tables_to_restore)} tables\", \"INFO\")\n",
    "            return tables_to_restore\n",
    "        else:\n",
    "            # Scan backup directory\n",
    "            try:\n",
    "                backup_contents = dbutils.fs.ls(restore_source_path)\n",
    "                tables_to_restore = [item.name.rstrip('/') for item in backup_contents \n",
    "                                     if not item.name.startswith('_') and not item.name.startswith('.') and item.isDir]\n",
    "                log_message(f\"Directory scan: Found {len(tables_to_restore)} tables\", \"INFO\")\n",
    "                return tables_to_restore\n",
    "            except Exception as e:\n",
    "                log_message(f\"Error listing backup contents: {str(e)}\", \"ERROR\")\n",
    "                return []\n",
    "\n",
    "def validate_backup_integrity(restore_source_path, tables_to_restore):\n",
    "    \"\"\"Validate backup integrity before restore\"\"\"\n",
    "    log_message(\"Validating backup integrity...\", \"INFO\")\n",
    "    \n",
    "    validation_results = {}\n",
    "    all_valid = True\n",
    "    \n",
    "    for table in tables_to_restore:\n",
    "        try:\n",
    "            table_path = f\"{restore_source_path}/{table}\"\n",
    "            \n",
    "            # Check if table directory exists\n",
    "            dbutils.fs.ls(table_path)\n",
    "            \n",
    "            # Try to read the table\n",
    "            df = spark.read.format(\"delta\").load(table_path)\n",
    "            row_count = df.count()\n",
    "            \n",
    "            validation_results[table] = {\n",
    "                \"valid\": True,\n",
    "                \"row_count\": row_count\n",
    "            }\n",
    "            \n",
    "            log_message(f\"Table {table}: Valid ({row_count} rows)\", \"INFO\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            all_valid = False\n",
    "            validation_results[table] = {\n",
    "                \"valid\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            log_message(f\"Table {table}: Invalid - {str(e)}\", \"ERROR\")\n",
    "    \n",
    "    if all_valid:\n",
    "        log_message(\"Backup integrity validation passed\", \"INFO\")\n",
    "    else:\n",
    "        log_message(\"Backup integrity validation failed for some tables\", \"WARNING\")\n",
    "    \n",
    "    return all_valid, validation_results\n",
    "\n",
    "def verify_restore_result(source_path, target_path, tables):\n",
    "    \"\"\"Verify restore results by comparing row counts\"\"\"\n",
    "    log_message(\"Verifying restore results...\", \"INFO\")\n",
    "    \n",
    "    verification_results = {}\n",
    "    all_verified = True\n",
    "    \n",
    "    for table in tables:\n",
    "        try:\n",
    "            source_table_path = f\"{source_path}/{table}\"\n",
    "            target_table_path = f\"{target_path}/{table}\"\n",
    "            \n",
    "            # Count rows in source backup and restored table\n",
    "            source_count = spark.read.format(\"delta\").load(source_table_path).count()\n",
    "            target_count = spark.read.format(\"delta\").load(target_table_path).count()\n",
    "            \n",
    "            rows_match = source_count == target_count\n",
    "            \n",
    "            if not rows_match:\n",
    "                all_verified = False\n",
    "            \n",
    "            verification_results[table] = {\n",
    "                \"verified\": rows_match,\n",
    "                \"backup_rows\": source_count,\n",
    "                \"restored_rows\": target_count\n",
    "            }\n",
    "            \n",
    "            log_message(f\"Table {table} verification: {'SUCCESS' if rows_match else 'FAILED'}\", \"INFO\")\n",
    "            if not rows_match:\n",
    "                log_message(f\"  - Backup rows: {source_count}, Restored rows: {target_count}\", \"WARNING\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            all_verified = False\n",
    "            verification_results[table] = {\n",
    "                \"verified\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            log_message(f\"Error verifying table {table}: {str(e)}\", \"ERROR\")\n",
    "    \n",
    "    if all_verified:\n",
    "        log_message(\"Restore verification completed successfully\", \"INFO\")\n",
    "    else:\n",
    "        log_message(\"Restore verification failed for one or more tables\", \"WARNING\")\n",
    "    \n",
    "    return all_verified, verification_results\n",
    "\n",
    "print(\"Restore helper functions defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0839ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Main Restore Function\n",
    "# ============================================================================\n",
    "\n",
    "def execute_restore_operation():\n",
    "    \"\"\"Execute the main restore operation\"\"\"\n",
    "    \n",
    "    start_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_message(\"Starting Microsoft Fabric lakehouse restore process\", \"INFO\")\n",
    "    \n",
    "    try:\n",
    "        # Validate parameters\n",
    "        validate_restore_parameters()\n",
    "        \n",
    "        # Get configuration values\n",
    "        restore_source_path = dbutils.widgets.get(\"restore_source_path\")\n",
    "        target_lakehouse = dbutils.widgets.get(\"restore_target_lakehouse\")\n",
    "        overwrite_existing = dbutils.widgets.get(\"overwrite_existing\") == \"True\"\n",
    "        restore_delta_logs = dbutils.widgets.get(\"restore_delta_logs\") == \"True\"\n",
    "        validate_before_restore = dbutils.widgets.get(\"validate_before_restore\") == \"True\"\n",
    "        verify_after_restore = dbutils.widgets.get(\"verify_after_restore\") == \"True\"\n",
    "        \n",
    "        # Construct target path\n",
    "        target_path = get_target_path()\n",
    "        \n",
    "        log_message(f\"Restore source: {restore_source_path}\", \"INFO\")\n",
    "        log_message(f\"Restore target: {target_path}\", \"INFO\")\n",
    "        log_message(f\"Target lakehouse: {target_lakehouse}\", \"INFO\")\n",
    "        \n",
    "        # Read backup manifest if available\n",
    "        manifest_data = read_backup_manifest(restore_source_path)\n",
    "        \n",
    "        # Get list of tables to restore\n",
    "        tables_to_restore = get_tables_to_restore(restore_source_path, manifest_data)\n",
    "        \n",
    "        if not tables_to_restore:\n",
    "            log_message(\"No tables found to restore\", \"WARNING\")\n",
    "            return {\n",
    "                \"status\": \"warning\",\n",
    "                \"message\": \"No tables found to restore\",\n",
    "                \"tables_restored\": 0\n",
    "            }\n",
    "        \n",
    "        log_message(f\"Tables to restore: {', '.join(tables_to_restore)}\", \"INFO\")\n",
    "        \n",
    "        # Validate backup integrity if requested\n",
    "        if validate_before_restore:\n",
    "            integrity_valid, validation_results = validate_backup_integrity(restore_source_path, tables_to_restore)\n",
    "            if not integrity_valid:\n",
    "                log_message(\"Backup integrity validation failed. Restore aborted.\", \"ERROR\")\n",
    "                return {\n",
    "                    \"status\": \"failed\",\n",
    "                    \"error\": \"Backup integrity validation failed\",\n",
    "                    \"validation_results\": validation_results\n",
    "                }\n",
    "        \n",
    "        # Restore each table\n",
    "        restored_count = 0\n",
    "        failed_tables = []\n",
    "        \n",
    "        for table in tables_to_restore:\n",
    "            try:\n",
    "                log_message(f\"Restoring table: {table}\", \"INFO\")\n",
    "                \n",
    "                # Read from backup\n",
    "                backup_table_path = f\"{restore_source_path}/{table}\"\n",
    "                backup_df = spark.read.format(\"delta\").load(backup_table_path)\n",
    "                \n",
    "                # Write to target\n",
    "                target_table_path = f\"{target_path}/{table}\"\n",
    "                \n",
    "                if overwrite_existing:\n",
    "                    backup_df.write.format(\"delta\").mode(\"overwrite\").save(target_table_path)\n",
    "                else:\n",
    "                    # Check if table exists first\n",
    "                    try:\n",
    "                        existing_df = spark.read.format(\"delta\").load(target_table_path)\n",
    "                        log_message(f\"Table {table} already exists. Skipping (overwrite disabled).\", \"WARNING\")\n",
    "                        continue\n",
    "                    except:\n",
    "                        # Table doesn't exist, proceed with create\n",
    "                        backup_df.write.format(\"delta\").mode(\"overwrite\").save(target_table_path)\n",
    "                \n",
    "                log_message(f\"Data restored for table: {table}\", \"INFO\")\n",
    "                \n",
    "                # Restore Delta logs if requested\n",
    "                if restore_delta_logs:\n",
    "                    try:\n",
    "                        backup_log_path = f\"{backup_table_path}/_delta_log\"\n",
    "                        target_log_path = f\"{target_table_path}/_delta_log\"\n",
    "                        \n",
    "                        # Check if backup logs exist\n",
    "                        dbutils.fs.ls(backup_log_path)\n",
    "                        log_message(f\"Restoring Delta logs for table {table}\", \"INFO\")\n",
    "                        dbutils.fs.cp(backup_log_path, target_log_path, True)\n",
    "                        log_message(f\"Delta logs restored for table: {table}\", \"INFO\")\n",
    "                        \n",
    "                    except Exception as log_error:\n",
    "                        log_message(f\"Delta logs not found or error restoring for table {table}: {str(log_error)}\", \"WARNING\")\n",
    "                \n",
    "                log_message(f\"Completed restore of table: {table}\", \"INFO\")\n",
    "                restored_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_message(f\"Error restoring table {table}: {str(e)}\", \"ERROR\")\n",
    "                failed_tables.append(table)\n",
    "        \n",
    "        # Record end time\n",
    "        end_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        duration_seconds = (datetime.datetime.strptime(end_time, \"%Y-%m-%d %H:%M:%S\") - \n",
    "                           datetime.datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")).total_seconds()\n",
    "        \n",
    "        # Verify restore if requested\n",
    "        verification_results = None\n",
    "        if verify_after_restore and restored_count > 0:\n",
    "            successfully_restored = [t for t in tables_to_restore if t not in failed_tables]\n",
    "            verification_passed, verification_results = verify_restore_result(\n",
    "                restore_source_path, target_path, successfully_restored\n",
    "            )\n",
    "        \n",
    "        # Final summary\n",
    "        log_message(\"\", \"INFO\")\n",
    "        log_message(\"=== RESTORE SUMMARY ===\", \"INFO\")\n",
    "        log_message(f\"Restore completed at: {end_time}\", \"INFO\")\n",
    "        log_message(f\"Source: {restore_source_path}\", \"INFO\")\n",
    "        log_message(f\"Target: {target_path}\", \"INFO\")\n",
    "        log_message(f\"Tables requested: {len(tables_to_restore)}\", \"INFO\")\n",
    "        log_message(f\"Tables restored: {restored_count}\", \"INFO\")\n",
    "        log_message(f\"Tables failed: {len(failed_tables)}\", \"INFO\")\n",
    "        if failed_tables:\n",
    "            log_message(f\"Failed tables: {', '.join(failed_tables)}\", \"WARNING\")\n",
    "        log_message(f\"Duration: {duration_seconds:.2f} seconds\", \"INFO\")\n",
    "        log_message(\"======================\", \"INFO\")\n",
    "        \n",
    "        # Return results\n",
    "        result = {\n",
    "            \"status\": \"success\" if len(failed_tables) == 0 else \"partial_success\",\n",
    "            \"restore_source\": restore_source_path,\n",
    "            \"restore_target\": target_path,\n",
    "            \"tables_requested\": len(tables_to_restore),\n",
    "            \"tables_restored\": restored_count,\n",
    "            \"tables_failed\": len(failed_tables),\n",
    "            \"failed_tables\": failed_tables,\n",
    "            \"duration_seconds\": duration_seconds,\n",
    "            \"verification_results\": verification_results\n",
    "        }\n",
    "        \n",
    "        if len(failed_tables) == 0:\n",
    "            log_message(\"Restore operation completed successfully\", \"INFO\")\n",
    "        else:\n",
    "            log_message(f\"Restore operation completed with {len(failed_tables)} failures\", \"WARNING\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Restore operation failed with error: {str(e)}\", \"ERROR\")\n",
    "        import traceback\n",
    "        log_message(f\"Full error trace: {traceback.format_exc()}\", \"ERROR\")\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "\n",
    "print(\"Main restore function defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc8b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Execute Restore Operation\n",
    "# ============================================================================\n",
    "\n",
    "# Execute the restore operation\n",
    "print(\"Starting restore operation...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "restore_result = execute_restore_operation()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Restore operation completed.\")\n",
    "print(f\"Final status: {restore_result['status']}\")\n",
    "\n",
    "# Display results in notebook output\n",
    "if restore_result['status'] == 'success':\n",
    "    print(f\"✅ SUCCESS: All {restore_result['tables_restored']} tables restored successfully\")\n",
    "elif restore_result['status'] == 'partial_success':\n",
    "    print(f\"⚠️  PARTIAL SUCCESS: {restore_result['tables_restored']} tables restored, {restore_result['tables_failed']} failed\")\n",
    "    if restore_result['failed_tables']:\n",
    "        print(f\"Failed tables: {', '.join(restore_result['failed_tables'])}\")\n",
    "else:\n",
    "    print(f\"❌ FAILED: {restore_result.get('error', 'Unknown error')}\")\n",
    "\n",
    "# Exit with results for pipeline integration\n",
    "dbutils.notebook.exit(restore_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9945b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Additional Utility Functions (Optional)\n",
    "# ============================================================================\n",
    "\n",
    "# These functions can be used for advanced restore scenarios\n",
    "# Uncomment and run separately as needed\n",
    "\n",
    "def list_available_backups(backup_base_path):\n",
    "    \"\"\"\n",
    "    List all available backups in a backup location\n",
    "    Usage: Run this cell separately to explore available backups\n",
    "    \"\"\"\n",
    "    try:\n",
    "        backup_folders = dbutils.fs.ls(backup_base_path)\n",
    "        backups = []\n",
    "        \n",
    "        for folder in backup_folders:\n",
    "            folder_name = folder.name.rstrip('/')\n",
    "            if folder_name.startswith(\"backup_\") and folder.isDir:\n",
    "                try:\n",
    "                    # Try to read manifest\n",
    "                    manifest_path = f\"{backup_base_path}/{folder_name}/_manifest\"\n",
    "                    manifest_df = spark.read.format(\"json\").load(manifest_path)\n",
    "                    manifest_data = manifest_df.collect()[0].asDict()\n",
    "                    \n",
    "                    backups.append({\n",
    "                        \"folder\": folder_name,\n",
    "                        \"full_path\": f\"{backup_base_path}/{folder_name}\",\n",
    "                        \"backup_id\": manifest_data.get('backup_id', 'Unknown'),\n",
    "                        \"timestamp\": manifest_data.get('backup_timestamp', 'Unknown'),\n",
    "                        \"source_lakehouse\": manifest_data.get('source_lakehouse_name', 'Unknown'),\n",
    "                        \"tables_count\": len(manifest_data.get('tables', [])),\n",
    "                        \"size_mb\": manifest_data.get('backup_size_bytes', 0) / (1024*1024)\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    # Backup without manifest or invalid manifest\n",
    "                    backups.append({\n",
    "                        \"folder\": folder_name,\n",
    "                        \"full_path\": f\"{backup_base_path}/{folder_name}\",\n",
    "                        \"backup_id\": \"Unknown\",\n",
    "                        \"timestamp\": \"Unknown\",\n",
    "                        \"source_lakehouse\": \"Unknown\",\n",
    "                        \"tables_count\": \"Unknown\",\n",
    "                        \"size_mb\": \"Unknown\",\n",
    "                        \"error\": str(e)\n",
    "                    })\n",
    "        \n",
    "        return backups\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing backups: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def compare_backup_tables(backup_path1, backup_path2):\n",
    "    \"\"\"\n",
    "    Compare tables between two backups\n",
    "    Usage: Run this cell separately to compare backup contents\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get tables from first backup\n",
    "        tables1 = set()\n",
    "        try:\n",
    "            contents1 = dbutils.fs.ls(backup_path1)\n",
    "            tables1 = {item.name.rstrip('/') for item in contents1 \n",
    "                      if not item.name.startswith('_') and not item.name.startswith('.') and item.isDir}\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading backup 1: {str(e)}\")\n",
    "        \n",
    "        # Get tables from second backup\n",
    "        tables2 = set()\n",
    "        try:\n",
    "            contents2 = dbutils.fs.ls(backup_path2)\n",
    "            tables2 = {item.name.rstrip('/') for item in contents2 \n",
    "                      if not item.name.startswith('_') and not item.name.startswith('.') and item.isDir}\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading backup 2: {str(e)}\")\n",
    "        \n",
    "        # Compare\n",
    "        common_tables = tables1.intersection(tables2)\n",
    "        only_in_1 = tables1 - tables2\n",
    "        only_in_2 = tables2 - tables1\n",
    "        \n",
    "        print(f\"Backup 1: {len(tables1)} tables\")\n",
    "        print(f\"Backup 2: {len(tables2)} tables\")\n",
    "        print(f\"Common tables: {len(common_tables)}\")\n",
    "        print(f\"Only in backup 1: {len(only_in_1)}\")\n",
    "        print(f\"Only in backup 2: {len(only_in_2)}\")\n",
    "        \n",
    "        if only_in_1:\n",
    "            print(f\"Tables only in backup 1: {', '.join(sorted(only_in_1))}\")\n",
    "        if only_in_2:\n",
    "            print(f\"Tables only in backup 2: {', '.join(sorted(only_in_2))}\")\n",
    "        \n",
    "        return {\n",
    "            \"backup1_tables\": tables1,\n",
    "            \"backup2_tables\": tables2,\n",
    "            \"common_tables\": common_tables,\n",
    "            \"only_in_backup1\": only_in_1,\n",
    "            \"only_in_backup2\": only_in_2\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error comparing backups: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Uncomment these lines to use the utility functions:\n",
    "# backup_base = \"abfss://backups@mystorageaccount.dfs.core.windows.net\"\n",
    "# available_backups = list_available_backups(backup_base)\n",
    "# print(f\"Found {len(available_backups)} backups\")\n",
    "\n",
    "print(\"Utility functions defined. Uncomment and modify the example lines above to use them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dad8a2c",
   "metadata": {},
   "source": [
    "# Restore Notebook Completion\n",
    "\n",
    "## Usage Instructions:\n",
    "\n",
    "1. **Configure Parameters**: Set the required parameters in the first cell according to your restore requirements\n",
    "2. **Run All Cells**: Execute all cells to perform the restore operation\n",
    "3. **Review Results**: Check the summary output for restore status and any issues\n",
    "\n",
    "## Use Cases:\n",
    "- **Disaster Recovery**: Restore entire lakehouses from backups\n",
    "- **Point-in-Time Restore**: Restore to a specific backup version\n",
    "- **Environment Synchronization**: Sync data between dev/test/prod environments\n",
    "- **Data Migration**: Move data between workspaces\n",
    "\n",
    "## For Other Operations:\n",
    "- **Creating Backups**: Use the `Fabric_Lakehouse_Backup.ipynb` notebook\n",
    "- **Cleanup Operations**: Use the `Fabric_Lakehouse_Cleanup.ipynb` notebook\n",
    "\n",
    "## Utility Functions:\n",
    "The last cell contains optional utility functions for exploring available backups and comparing backup contents. Uncomment and modify the example lines to use them."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
