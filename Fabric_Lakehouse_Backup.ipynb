{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c2336ae",
   "metadata": {},
   "source": [
    "# Microsoft Fabric Lakehouse Backup Notebook\n",
    "\n",
    "This notebook implements a parameterized full backup solution for Microsoft Fabric Lakehouses.\n",
    "\n",
    "## Features:\n",
    "- Full backup of all Delta tables in the source Lakehouse\n",
    "- Configurable backup destinations (Storage Account, another Lakehouse, ADLS)\n",
    "- Metadata preservation including Delta transaction logs\n",
    "- Backup verification with row count validation\n",
    "- Detailed logging and error handling\n",
    "- Automatic cleanup based on retention policy\n",
    "\n",
    "## Compatible with Microsoft Fabric scheduling via:\n",
    "- Direct notebook scheduling\n",
    "- Data Factory pipeline integration\n",
    "- REST API execution\n",
    "\n",
    "**Author:** Generated for Microsoft Fabric  \n",
    "**Version:** 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da964dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: Backup Configuration Parameters\n",
    "# ============================================================================\n",
    "\n",
    "# Configure these parameters before running or when scheduling the notebook\n",
    "dbutils.widgets.text(\"source_lakehouse_name\", \"\", \"Source Lakehouse Name\")\n",
    "dbutils.widgets.text(\"source_workspace_id\", \"\", \"Source Workspace ID (leave empty for current workspace)\")\n",
    "dbutils.widgets.dropdown(\"backup_type\", \"storage_account\", [\"storage_account\", \"lakehouse\", \"adls\"], \"Backup Destination Type\")\n",
    "dbutils.widgets.text(\"backup_storage_account\", \"\", \"Backup Storage Account Name (for storage_account type)\")\n",
    "dbutils.widgets.text(\"backup_container\", \"lakehouse-backups\", \"Backup Container Name\")\n",
    "dbutils.widgets.text(\"backup_lakehouse_name\", \"\", \"Backup Lakehouse Name (for lakehouse type)\")\n",
    "dbutils.widgets.text(\"backup_workspace_id\", \"\", \"Backup Workspace ID (leave empty for current workspace)\")\n",
    "dbutils.widgets.text(\"backup_adls_account\", \"\", \"Backup ADLS Account Name (for adls type)\")\n",
    "dbutils.widgets.text(\"backup_adls_container\", \"\", \"Backup ADLS Container Name (for adls type)\")\n",
    "dbutils.widgets.text(\"backup_folder_path\", \"\", \"Custom Backup Folder Path (leave empty for auto-generated)\")\n",
    "dbutils.widgets.text(\"retention_days\", \"30\", \"Backup Retention Period in Days\")\n",
    "dbutils.widgets.dropdown(\"verify_backup\", \"True\", [\"True\", \"False\"], \"Verify Backup After Completion\")\n",
    "dbutils.widgets.dropdown(\"include_delta_logs\", \"True\", [\"True\", \"False\"], \"Include Delta Transaction Logs\")\n",
    "dbutils.widgets.dropdown(\"compress_backup\", \"True\", [\"True\", \"False\"], \"Compress Backup Files\")\n",
    "dbutils.widgets.dropdown(\"enable_detailed_logging\", \"True\", [\"True\", \"False\"], \"Enable Detailed Logging\")\n",
    "dbutils.widgets.dropdown(\"use_managed_identity\", \"True\", [\"True\", \"False\"], \"Use Managed Identity for External Storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813572be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Import Required Libraries\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import uuid\n",
    "from pyspark.sql.functions import lit, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n",
    "from delta.tables import DeltaTable\n",
    "import time\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Backup process initiated at: {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c490f3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Core Helper Functions\n",
    "# ============================================================================\n",
    "\n",
    "def get_current_timestamp():\n",
    "    \"\"\"Return current timestamp in a standardized format\"\"\"\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "def get_backup_path():\n",
    "    \"\"\"Generate backup path based on timestamp and a unique identifier\"\"\"\n",
    "    timestamp = get_current_timestamp()\n",
    "    backup_id = str(uuid.uuid4())[:8]\n",
    "    return f\"backup_{timestamp}_{backup_id}\"\n",
    "\n",
    "def log_message(message, level=\"INFO\"):\n",
    "    \"\"\"Log a message with timestamp and level\"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{timestamp}] [{level}] {message}\")\n",
    "    \n",
    "    # If detailed logging is enabled, also log to a Delta table\n",
    "    if enable_detailed_logging and level != \"DEBUG\":\n",
    "        log_entry = [(timestamp, level, message)]\n",
    "        log_schema = StructType([\n",
    "            StructField(\"timestamp\", StringType(), False),\n",
    "            StructField(\"level\", StringType(), False),\n",
    "            StructField(\"message\", StringType(), False)\n",
    "        ])\n",
    "        log_df = spark.createDataFrame(log_entry, log_schema)\n",
    "        \n",
    "        # Write to the log table\n",
    "        if backup_type == \"lakehouse\":\n",
    "            try:\n",
    "                log_df.write.format(\"delta\").mode(\"append\").save(f\"{backup_base_path}/_logs\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not write to log table: {str(e)}\")\n",
    "        else:\n",
    "            # For other backup types, keep in-memory log and write at the end\n",
    "            global log_entries\n",
    "            log_entries.append((timestamp, level, message))\n",
    "\n",
    "def validate_parameters():\n",
    "    \"\"\"Validate that required parameters are provided\"\"\"\n",
    "    source_lakehouse_name = dbutils.widgets.get(\"source_lakehouse_name\")\n",
    "    backup_type = dbutils.widgets.get(\"backup_type\")\n",
    "    \n",
    "    if not source_lakehouse_name:\n",
    "        raise ValueError(\"Source Lakehouse Name is required\")\n",
    "    \n",
    "    if backup_type == \"storage_account\":\n",
    "        if not dbutils.widgets.get(\"backup_storage_account\"):\n",
    "            raise ValueError(\"Backup Storage Account is required for storage_account backup type\")\n",
    "    elif backup_type == \"lakehouse\":\n",
    "        if not dbutils.widgets.get(\"backup_lakehouse_name\"):\n",
    "            raise ValueError(\"Backup Lakehouse Name is required for lakehouse backup type\")\n",
    "    elif backup_type == \"adls\":\n",
    "        if not dbutils.widgets.get(\"backup_adls_account\") or not dbutils.widgets.get(\"backup_adls_container\"):\n",
    "            raise ValueError(\"Backup ADLS Account and Container are required for adls backup type\")\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid backup type: {backup_type}\")\n",
    "\n",
    "print(\"Core helper functions defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ba94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Path and Authentication Functions\n",
    "# ============================================================================\n",
    "\n",
    "def get_source_path():\n",
    "    \"\"\"Construct the path to the source Lakehouse\"\"\"\n",
    "    source_lakehouse_name = dbutils.widgets.get(\"source_lakehouse_name\")\n",
    "    source_workspace_id = dbutils.widgets.get(\"source_workspace_id\")\n",
    "    \n",
    "    if source_workspace_id and source_workspace_id.strip():\n",
    "        # External lakehouse access via OneLake path\n",
    "        return f\"abfss://{source_workspace_id}@onelake.dfs.fabric.microsoft.com/{source_lakehouse_name}.Lakehouse/Tables\"\n",
    "    else:\n",
    "        # Current workspace lakehouse\n",
    "        return \"Tables\"\n",
    "\n",
    "def setup_external_storage_auth():\n",
    "    \"\"\"Setup authentication for external storage if needed\"\"\"\n",
    "    use_managed_identity = dbutils.widgets.get(\"use_managed_identity\") == \"True\"\n",
    "    backup_type = dbutils.widgets.get(\"backup_type\")\n",
    "    \n",
    "    if backup_type in [\"storage_account\", \"adls\"] and use_managed_identity:\n",
    "        log_message(\"Configuring managed identity for external storage authentication\", \"INFO\")\n",
    "        \n",
    "        # Configure OAuth provider for external storage\n",
    "        if backup_type == \"storage_account\":\n",
    "            account_name = dbutils.widgets.get(\"backup_storage_account\")\n",
    "            spark.conf.set(f\"fs.azure.account.auth.type.{account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "            spark.conf.set(f\"fs.azure.account.oauth.provider.type.{account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider\")\n",
    "        elif backup_type == \"adls\":\n",
    "            account_name = dbutils.widgets.get(\"backup_adls_account\")\n",
    "            spark.conf.set(f\"fs.azure.account.auth.type.{account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "            spark.conf.set(f\"fs.azure.account.oauth.provider.type.{account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider\")\n",
    "\n",
    "def get_backup_base_path():\n",
    "    \"\"\"Construct the base path for the backup based on the backup type\"\"\"\n",
    "    backup_type = dbutils.widgets.get(\"backup_type\")\n",
    "    \n",
    "    if backup_type == \"storage_account\":\n",
    "        account_name = dbutils.widgets.get(\"backup_storage_account\")\n",
    "        container_name = dbutils.widgets.get(\"backup_container\")\n",
    "        return f\"abfss://{container_name}@{account_name}.dfs.core.windows.net\"\n",
    "    \n",
    "    elif backup_type == \"lakehouse\":\n",
    "        lakehouse_name = dbutils.widgets.get(\"backup_lakehouse_name\")\n",
    "        backup_workspace_id = dbutils.widgets.get(\"backup_workspace_id\")\n",
    "        \n",
    "        if backup_workspace_id and backup_workspace_id.strip():\n",
    "            # External workspace lakehouse\n",
    "            return f\"abfss://{backup_workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_name}.Lakehouse/Files/lakehouse_backups\"\n",
    "        else:\n",
    "            # Current workspace lakehouse - use Files folder\n",
    "            return f\"Files/lakehouse_backups\"\n",
    "    \n",
    "    elif backup_type == \"adls\":\n",
    "        account_name = dbutils.widgets.get(\"backup_adls_account\")\n",
    "        container_name = dbutils.widgets.get(\"backup_adls_container\")\n",
    "        return f\"abfss://{container_name}@{account_name}.dfs.core.windows.net\"\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Invalid backup type: {backup_type}\")\n",
    "\n",
    "def get_backup_folder():\n",
    "    \"\"\"Get the backup folder path, either user-specified or auto-generated\"\"\"\n",
    "    user_path = dbutils.widgets.get(\"backup_folder_path\")\n",
    "    if user_path and user_path.strip():\n",
    "        return user_path.strip()\n",
    "    else:\n",
    "        return get_backup_path()\n",
    "\n",
    "print(\"Path and authentication functions defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7353322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Table Discovery and Backup Operations\n",
    "# ============================================================================\n",
    "\n",
    "def list_tables_in_lakehouse(source_path):\n",
    "    \"\"\"List all tables in the lakehouse\"\"\"\n",
    "    try:\n",
    "        # Try to list using dbutils first\n",
    "        table_list = dbutils.fs.ls(source_path)\n",
    "        tables = [t.name.rstrip('/') for t in table_list \n",
    "                 if not t.name.startswith('_') and not t.name.startswith('.') and t.isDir]\n",
    "        return tables\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error listing tables with dbutils: {str(e)}\", \"WARNING\")\n",
    "        \n",
    "        # Fallback: try using Spark SQL\n",
    "        try:\n",
    "            if source_path == \"Tables\":\n",
    "                # Current lakehouse\n",
    "                tables_df = spark.sql(\"SHOW TABLES\")\n",
    "                return [row.tableName for row in tables_df.collect()]\n",
    "            else:\n",
    "                # External lakehouse - we'll need to scan the directory differently\n",
    "                log_message(\"External lakehouse detected, using alternative table discovery\", \"INFO\")\n",
    "                return []\n",
    "        except Exception as e2:\n",
    "            log_message(f\"Error listing tables with SQL: {str(e2)}\", \"ERROR\")\n",
    "            return []\n",
    "\n",
    "def create_backup_manifest(backup_path, tables, files_count, backup_size_bytes, start_time, end_time):\n",
    "    \"\"\"Create a manifest file with backup metadata\"\"\"\n",
    "    manifest = {\n",
    "        \"backup_id\": backup_path.split('_')[-1] if '_' in backup_path else str(uuid.uuid4())[:8],\n",
    "        \"backup_timestamp\": get_current_timestamp(),\n",
    "        \"source_lakehouse_name\": dbutils.widgets.get(\"source_lakehouse_name\"),\n",
    "        \"source_workspace_id\": dbutils.widgets.get(\"source_workspace_id\"),\n",
    "        \"backup_type\": dbutils.widgets.get(\"backup_type\"),\n",
    "        \"tables\": tables,\n",
    "        \"files_count\": files_count,\n",
    "        \"backup_size_bytes\": backup_size_bytes,\n",
    "        \"include_delta_logs\": dbutils.widgets.get(\"include_delta_logs\") == \"True\",\n",
    "        \"compressed\": dbutils.widgets.get(\"compress_backup\") == \"True\",\n",
    "        \"start_time\": start_time,\n",
    "        \"end_time\": end_time,\n",
    "        \"duration_seconds\": (datetime.datetime.strptime(end_time, \"%Y-%m-%d %H:%M:%S\") - \n",
    "                             datetime.datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")).total_seconds(),\n",
    "        \"fabric_version\": \"1.0\"\n",
    "    }\n",
    "    \n",
    "    # Write manifest to backup location\n",
    "    try:\n",
    "        manifest_df = spark.createDataFrame([manifest])\n",
    "        manifest_df.write.mode(\"overwrite\").format(\"json\").save(f\"{backup_base_path}/{backup_folder}/_manifest\")\n",
    "        log_message(\"Backup manifest created successfully\", \"INFO\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error creating backup manifest: {str(e)}\", \"ERROR\")\n",
    "    \n",
    "    return manifest\n",
    "\n",
    "def verify_backup(source_path, backup_path, tables):\n",
    "    \"\"\"Verify the backup by comparing row counts between source and backup\"\"\"\n",
    "    log_message(\"Starting backup verification...\", \"INFO\")\n",
    "    \n",
    "    verification_results = {}\n",
    "    all_verified = True\n",
    "    \n",
    "    for table in tables:\n",
    "        try:\n",
    "            source_table_path = f\"{source_path}/{table}\"\n",
    "            backup_table_path = f\"{backup_path}/{table}\"\n",
    "            \n",
    "            # Count rows in source and backup\n",
    "            try:\n",
    "                source_count = spark.read.format(\"delta\").load(source_table_path).count()\n",
    "                backup_count = spark.read.format(\"delta\").load(backup_table_path).count()\n",
    "                \n",
    "                # Basic verification\n",
    "                rows_match = source_count == backup_count\n",
    "                verified = rows_match\n",
    "                \n",
    "                if not verified:\n",
    "                    all_verified = False\n",
    "                    \n",
    "                verification_results[table] = {\n",
    "                    \"verified\": verified,\n",
    "                    \"source_rows\": source_count,\n",
    "                    \"backup_rows\": backup_count\n",
    "                }\n",
    "                \n",
    "                log_message(f\"Table {table} verification: {'SUCCESS' if verified else 'FAILED'}\", \"INFO\")\n",
    "                if not verified:\n",
    "                    log_message(f\"  - Source rows: {source_count}, Backup rows: {backup_count}\", \"WARNING\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                all_verified = False\n",
    "                verification_results[table] = {\n",
    "                    \"verified\": False,\n",
    "                    \"error\": f\"Row count comparison failed: {str(e)}\"\n",
    "                }\n",
    "                log_message(f\"Error verifying table {table}: {str(e)}\", \"ERROR\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            all_verified = False\n",
    "            verification_results[table] = {\n",
    "                \"verified\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            log_message(f\"Error verifying table {table}: {str(e)}\", \"ERROR\")\n",
    "    \n",
    "    # Write verification results to backup location\n",
    "    try:\n",
    "        verification_df = spark.createDataFrame([{\"results\": json.dumps(verification_results), \"all_verified\": all_verified}])\n",
    "        verification_df.write.mode(\"overwrite\").format(\"json\").save(f\"{backup_base_path}/{backup_folder}/_verification\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error writing verification results: {str(e)}\", \"WARNING\")\n",
    "    \n",
    "    if all_verified:\n",
    "        log_message(\"Backup verification completed successfully\", \"INFO\")\n",
    "    else:\n",
    "        log_message(\"Backup verification failed for one or more tables\", \"WARNING\")\n",
    "    \n",
    "    return all_verified\n",
    "\n",
    "def cleanup_old_backups():\n",
    "    \"\"\"Clean up backups older than the retention period\"\"\"\n",
    "    retention_days = int(dbutils.widgets.get(\"retention_days\"))\n",
    "    if retention_days <= 0:\n",
    "        log_message(\"Backup retention disabled, skipping cleanup\", \"INFO\")\n",
    "        return\n",
    "    \n",
    "    log_message(f\"Checking for backups older than {retention_days} days...\", \"INFO\")\n",
    "    \n",
    "    backup_base = backup_base_path\n",
    "    \n",
    "    try:\n",
    "        # List folders in the backup location\n",
    "        backup_folders = dbutils.fs.ls(backup_base)\n",
    "        \n",
    "        deleted_count = 0\n",
    "        for folder in backup_folders:\n",
    "            try:\n",
    "                folder_name = folder.name.rstrip('/')\n",
    "                # Parse timestamp from folder name (backup_YYYY-MM-DD_HH-MM-SS_uuid)\n",
    "                if folder_name.startswith(\"backup_\") and folder.isDir:\n",
    "                    parts = folder_name.split(\"_\")\n",
    "                    if len(parts) >= 3:\n",
    "                        timestamp_str = parts[1] + \"_\" + parts[2]\n",
    "                        backup_timestamp = datetime.datetime.strptime(timestamp_str, \"%Y-%m-%d_%H-%M-%S\")\n",
    "                        \n",
    "                        # Calculate age in days\n",
    "                        age_days = (datetime.datetime.now() - backup_timestamp).days\n",
    "                        \n",
    "                        if age_days > retention_days:\n",
    "                            log_message(f\"Deleting backup {folder_name} ({age_days} days old)\", \"INFO\")\n",
    "                            dbutils.fs.rm(f\"{backup_base}/{folder_name}\", True)\n",
    "                            deleted_count += 1\n",
    "            except Exception as e:\n",
    "                log_message(f\"Error processing backup folder {folder.name}: {str(e)}\", \"ERROR\")\n",
    "        \n",
    "        log_message(f\"Cleanup completed. Deleted {deleted_count} old backups.\", \"INFO\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error listing backups for cleanup: {str(e)}\", \"ERROR\")\n",
    "\n",
    "print(\"Backup operation functions defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f870e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Main Backup Execution\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    # Initialize logging\n",
    "    global log_entries\n",
    "    log_entries = []\n",
    "    \n",
    "    start_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_message(\"Starting Microsoft Fabric lakehouse backup process\", \"INFO\")\n",
    "    \n",
    "    # Parse and validate parameters\n",
    "    validate_parameters()\n",
    "    \n",
    "    # Setup external storage authentication if needed\n",
    "    setup_external_storage_auth()\n",
    "    \n",
    "    # Get configuration values from widgets\n",
    "    source_lakehouse_name = dbutils.widgets.get(\"source_lakehouse_name\")\n",
    "    source_workspace_id = dbutils.widgets.get(\"source_workspace_id\")\n",
    "    backup_type = dbutils.widgets.get(\"backup_type\")\n",
    "    include_delta_logs = dbutils.widgets.get(\"include_delta_logs\") == \"True\"\n",
    "    verify_after_backup = dbutils.widgets.get(\"verify_backup\") == \"True\"\n",
    "    compress_backup = dbutils.widgets.get(\"compress_backup\") == \"True\"\n",
    "    enable_detailed_logging = dbutils.widgets.get(\"enable_detailed_logging\") == \"True\"\n",
    "    \n",
    "    # Construct paths\n",
    "    source_path = get_source_path()\n",
    "    backup_base_path = get_backup_base_path()\n",
    "    backup_folder = get_backup_folder()\n",
    "    backup_path = f\"{backup_base_path}/{backup_folder}\"\n",
    "    \n",
    "    log_message(f\"Source path: {source_path}\", \"INFO\")\n",
    "    log_message(f\"Backup path: {backup_path}\", \"INFO\")\n",
    "    \n",
    "    # Ensure backup location exists\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(backup_path)\n",
    "        log_message(\"Created backup directory\", \"INFO\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error creating backup directory: {str(e)}\", \"WARNING\")\n",
    "    \n",
    "    # Initialize backup stats\n",
    "    total_files = 0\n",
    "    total_size_bytes = 0\n",
    "    tables = []\n",
    "    \n",
    "    # Get list of tables in the source lakehouse\n",
    "    table_list = list_tables_in_lakehouse(source_path)\n",
    "    \n",
    "    if not table_list:\n",
    "        log_message(\"No tables found in source lakehouse\", \"WARNING\")\n",
    "    else:\n",
    "        log_message(f\"Found {len(table_list)} tables in source lakehouse: {', '.join(table_list)}\", \"INFO\")\n",
    "    \n",
    "    # Backup each table\n",
    "    for table in table_list:\n",
    "        try:\n",
    "            log_message(f\"Backing up table: {table}\", \"INFO\")\n",
    "            \n",
    "            # Read the source table\n",
    "            source_table_path = f\"{source_path}/{table}\"\n",
    "            source_df = spark.read.format(\"delta\").load(source_table_path)\n",
    "            \n",
    "            # Get table stats before backup (if possible)\n",
    "            try:\n",
    "                table_stats = spark.sql(f\"DESCRIBE DETAIL delta.`{source_table_path}`\").collect()[0]\n",
    "                table_files = table_stats.numFiles\n",
    "                table_size = table_stats.sizeInBytes\n",
    "                \n",
    "                # Update totals\n",
    "                total_files += table_files\n",
    "                total_size_bytes += table_size\n",
    "                \n",
    "                log_message(f\"Table {table} has {table_files} files and size {table_size} bytes\", \"INFO\")\n",
    "            except Exception as e:\n",
    "                log_message(f\"Could not get detailed stats for table {table}: {str(e)}\", \"WARNING\")\n",
    "                # Use row count as a basic metric\n",
    "                row_count = source_df.count()\n",
    "                log_message(f\"Table {table} has {row_count} rows\", \"INFO\")\n",
    "            \n",
    "            tables.append(table)\n",
    "            \n",
    "            # Backup options\n",
    "            write_options = {}\n",
    "            \n",
    "            if compress_backup:\n",
    "                write_options[\"compression\"] = \"snappy\"\n",
    "            \n",
    "            # Write to backup location\n",
    "            backup_table_path = f\"{backup_path}/{table}\"\n",
    "            source_df.write.format(\"delta\").options(**write_options).mode(\"overwrite\").save(backup_table_path)\n",
    "            \n",
    "            # Backup Delta logs if requested\n",
    "            if include_delta_logs:\n",
    "                try:\n",
    "                    log_message(f\"Copying Delta logs for table {table}\", \"INFO\")\n",
    "                    # Copy the _delta_log directory\n",
    "                    source_log_path = f\"{source_table_path}/_delta_log\"\n",
    "                    backup_log_path = f\"{backup_table_path}/_delta_log\"\n",
    "                    \n",
    "                    # Check if source logs exist\n",
    "                    try:\n",
    "                        dbutils.fs.ls(source_log_path)\n",
    "                        dbutils.fs.cp(source_log_path, backup_log_path, True)\n",
    "                        log_message(f\"Successfully copied Delta logs for table {table}\", \"INFO\")\n",
    "                    except Exception as log_error:\n",
    "                        log_message(f\"Delta logs not found or error copying for table {table}: {str(log_error)}\", \"WARNING\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    log_message(f\"Error copying Delta logs for table {table}: {str(e)}\", \"WARNING\")\n",
    "            \n",
    "            log_message(f\"Completed backup of table: {table}\", \"INFO\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            log_message(f\"Error backing up table {table}: {str(e)}\", \"ERROR\")\n",
    "    \n",
    "    # Record end time\n",
    "    end_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Create backup manifest\n",
    "    manifest = create_backup_manifest(\n",
    "        backup_folder, \n",
    "        tables, \n",
    "        total_files, \n",
    "        total_size_bytes, \n",
    "        start_time, \n",
    "        end_time\n",
    "    )\n",
    "    \n",
    "    # Write logs to backup location if not already done\n",
    "    if enable_detailed_logging and backup_type != \"lakehouse\":\n",
    "        try:\n",
    "            log_schema = StructType([\n",
    "                StructField(\"timestamp\", StringType(), False),\n",
    "                StructField(\"level\", StringType(), False),\n",
    "                StructField(\"message\", StringType(), False)\n",
    "            ])\n",
    "            \n",
    "            log_df = spark.createDataFrame(log_entries, log_schema)\n",
    "            log_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{backup_path}/_logs\")\n",
    "            log_message(\"Wrote logs to backup location\", \"INFO\")\n",
    "        except Exception as e:\n",
    "            log_message(f\"Error writing logs to backup location: {str(e)}\", \"WARNING\")\n",
    "    \n",
    "    # Verify backup if requested\n",
    "    if verify_after_backup and tables:\n",
    "        verification_result = verify_backup(source_path, backup_path, tables)\n",
    "        if verification_result:\n",
    "            log_message(\"Backup verification successful\", \"INFO\")\n",
    "        else:\n",
    "            log_message(\"Backup verification failed\", \"WARNING\")\n",
    "    \n",
    "    # Clean up old backups\n",
    "    try:\n",
    "        cleanup_old_backups()\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error during backup cleanup: {str(e)}\", \"WARNING\")\n",
    "    \n",
    "    # Final summary\n",
    "    duration_seconds = (datetime.datetime.strptime(end_time, \"%Y-%m-%d %H:%M:%S\") - \n",
    "                        datetime.datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")).total_seconds()\n",
    "    \n",
    "    log_message(\"\", \"INFO\")\n",
    "    log_message(\"=== BACKUP SUMMARY ===\", \"INFO\")\n",
    "    log_message(f\"Backup completed at: {end_time}\", \"INFO\")\n",
    "    log_message(f\"Backup location: {backup_path}\", \"INFO\")\n",
    "    log_message(f\"Tables backed up: {len(tables)}\", \"INFO\")\n",
    "    log_message(f\"Total files: {total_files}\", \"INFO\")\n",
    "    if total_size_bytes > 0:\n",
    "        log_message(f\"Total size: {total_size_bytes / (1024*1024):.2f} MB\", \"INFO\")\n",
    "    log_message(f\"Duration: {duration_seconds:.2f} seconds\", \"INFO\")\n",
    "    log_message(\"=====================\", \"INFO\")\n",
    "    \n",
    "    # Return success\n",
    "    dbutils.notebook.exit({\n",
    "        \"status\": \"success\",\n",
    "        \"backup_path\": backup_path,\n",
    "        \"tables_count\": len(tables),\n",
    "        \"files_count\": total_files,\n",
    "        \"size_mb\": total_size_bytes / (1024*1024) if total_size_bytes > 0 else 0,\n",
    "        \"duration_seconds\": duration_seconds,\n",
    "        \"verification_passed\": verification_result if verify_after_backup else None\n",
    "    })\n",
    "    \n",
    "except Exception as e:\n",
    "    log_message(f\"Backup failed with error: {str(e)}\", \"ERROR\")\n",
    "    import traceback\n",
    "    log_message(f\"Full error trace: {traceback.format_exc()}\", \"ERROR\")\n",
    "    \n",
    "    # Return failure\n",
    "    dbutils.notebook.exit({\n",
    "        \"status\": \"failed\",\n",
    "        \"error\": str(e),\n",
    "        \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784fb721",
   "metadata": {},
   "source": [
    "# Backup Notebook Completion\n",
    "\n",
    "## Usage Instructions:\n",
    "\n",
    "1. **Configure Parameters**: Set the required parameters in the first cell according to your backup requirements\n",
    "2. **Run All Cells**: Execute all cells to perform the backup operation\n",
    "3. **Schedule the Notebook**: \n",
    "   - Click the 'Schedule' button in the notebook toolbar\n",
    "   - Or add to a Data Factory pipeline\n",
    "   - Or use the REST API for programmatic execution\n",
    "\n",
    "## For Restore Operations:\n",
    "Use the separate `Fabric_Lakehouse_Restore.ipynb` notebook.\n",
    "\n",
    "## For Cleanup Operations:\n",
    "Use the separate `Fabric_Lakehouse_Cleanup.ipynb` notebook."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
