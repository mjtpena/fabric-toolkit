{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb6dd64",
   "metadata": {},
   "source": [
    "# Microsoft Fabric Lakehouse Backup Cleanup Notebook\n",
    "\n",
    "This notebook provides automated cleanup capabilities for Microsoft Fabric Lakehouse backups based on configurable retention policies.\n",
    "\n",
    "## Features:\n",
    "- Flexible retention policies (age-based, count-based, size-based)\n",
    "- Support for all backup storage types (Storage Account, Lakehouse, ADLS)\n",
    "- Cross-workspace backup cleanup\n",
    "- Dry-run mode for safe testing\n",
    "- Detailed logging and reporting\n",
    "- Backup preservation rules (keep daily, weekly, monthly)\n",
    "- Manual backup exclusion support\n",
    "\n",
    "## Use Cases:\n",
    "- Automated cleanup as part of backup strategy\n",
    "- Storage cost optimization\n",
    "- Compliance with data retention policies\n",
    "- Emergency cleanup for storage space\n",
    "\n",
    "Compatible with Microsoft Fabric scheduling for automated execution\n",
    "\n",
    "**Author:** Generated for Microsoft Fabric  \n",
    "**Version:** 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce46e67b",
   "metadata": {},
   "source": [
    "## 1. Cleanup Configuration Parameters\n",
    "\n",
    "Configure these parameters before running the cleanup operation. Review all settings carefully, especially when disabling dry_run mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4354c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure these parameters before running the cleanup operation\n",
    "dbutils.widgets.dropdown(\"cleanup_mode\", \"age_based\", [\"age_based\", \"count_based\", \"size_based\", \"advanced\"], \"Cleanup Mode\")\n",
    "dbutils.widgets.text(\"backup_location\", \"\", \"Backup Location Path (full path to backup directory)\")\n",
    "dbutils.widgets.text(\"backup_workspace_id\", \"\", \"Backup Workspace ID (if different from current)\")\n",
    "dbutils.widgets.text(\"retention_days\", \"30\", \"Maximum Age in Days (for age_based mode)\")\n",
    "dbutils.widgets.text(\"max_backup_count\", \"10\", \"Maximum Number of Backups to Keep (for count_based mode)\")\n",
    "dbutils.widgets.text(\"max_size_gb\", \"100\", \"Maximum Total Size in GB (for size_based mode)\")\n",
    "dbutils.widgets.dropdown(\"preserve_policy\", \"none\", [\"none\", \"daily\", \"weekly\", \"monthly\"], \"Preservation Policy\")\n",
    "dbutils.widgets.text(\"preserve_daily_count\", \"7\", \"Number of Daily Backups to Preserve\")\n",
    "dbutils.widgets.text(\"preserve_weekly_count\", \"4\", \"Number of Weekly Backups to Preserve\")\n",
    "dbutils.widgets.text(\"preserve_monthly_count\", \"12\", \"Number of Monthly Backups to Preserve\")\n",
    "dbutils.widgets.dropdown(\"dry_run\", \"True\", [\"True\", \"False\"], \"Dry Run Mode (preview only)\")\n",
    "dbutils.widgets.text(\"exclude_patterns\", \"\", \"Exclude Patterns (comma-separated, e.g., 'manual_*,prod_*')\")\n",
    "dbutils.widgets.text(\"include_patterns\", \"backup_*\", \"Include Patterns (comma-separated)\")\n",
    "dbutils.widgets.dropdown(\"use_managed_identity\", \"True\", [\"True\", \"False\"], \"Use Managed Identity for External Storage\")\n",
    "dbutils.widgets.dropdown(\"enable_detailed_logging\", \"True\", [\"True\", \"False\"], \"Enable Detailed Logging\")\n",
    "dbutils.widgets.dropdown(\"generate_report\", \"True\", [\"True\", \"False\"], \"Generate Cleanup Report\")\n",
    "\n",
    "print(\"Cleanup parameters configured. Ready to begin cleanup operation.\")\n",
    "print(\"IMPORTANT: Review parameters carefully before disabling dry_run mode.\")\n",
    "print(\"Cleanup modes available: age_based, count_based, size_based, advanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc76bf9",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "\n",
    "Import all necessary Python libraries for the cleanup operation including PySpark, Delta, and utility libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057bc523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import uuid\n",
    "import re\n",
    "from pyspark.sql.functions import lit, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, LongType, BooleanType\n",
    "from delta.tables import DeltaTable\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Cleanup process initiated at: {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c27fb0",
   "metadata": {},
   "source": [
    "## 3. Cleanup Helper Functions\n",
    "\n",
    "Define utility functions for logging, parameter validation, authentication setup, backup folder parsing, size calculation, manifest reading, and pattern matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd86dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_message(message, level=\"INFO\"):\n",
    "    \"\"\"Log a message with timestamp and level\"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{timestamp}] [{level}] {message}\")\n",
    "\n",
    "def validate_cleanup_parameters():\n",
    "    \"\"\"Validate that required cleanup parameters are provided\"\"\"\n",
    "    backup_location = dbutils.widgets.get(\"backup_location\")\n",
    "    cleanup_mode = dbutils.widgets.get(\"cleanup_mode\")\n",
    "    \n",
    "    if not backup_location:\n",
    "        raise ValueError(\"Backup location path is required\")\n",
    "    \n",
    "    # Validate cleanup mode specific parameters\n",
    "    if cleanup_mode == \"age_based\":\n",
    "        try:\n",
    "            retention_days = int(dbutils.widgets.get(\"retention_days\"))\n",
    "            if retention_days <= 0:\n",
    "                raise ValueError(\"Retention days must be positive\")\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Retention days must be a valid positive integer\")\n",
    "    \n",
    "    elif cleanup_mode == \"count_based\":\n",
    "        try:\n",
    "            max_count = int(dbutils.widgets.get(\"max_backup_count\"))\n",
    "            if max_count <= 0:\n",
    "                raise ValueError(\"Maximum backup count must be positive\")\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Maximum backup count must be a valid positive integer\")\n",
    "    \n",
    "    elif cleanup_mode == \"size_based\":\n",
    "        try:\n",
    "            max_size = float(dbutils.widgets.get(\"max_size_gb\"))\n",
    "            if max_size <= 0:\n",
    "                raise ValueError(\"Maximum size must be positive\")\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Maximum size must be a valid positive number\")\n",
    "    \n",
    "    # Check if backup location exists\n",
    "    try:\n",
    "        dbutils.fs.ls(backup_location)\n",
    "        log_message(f\"Backup location validated: {backup_location}\", \"INFO\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Backup location does not exist or is not accessible: {backup_location}\")\n",
    "\n",
    "def setup_external_storage_auth():\n",
    "    \"\"\"Setup authentication for external storage if needed\"\"\"\n",
    "    use_managed_identity = dbutils.widgets.get(\"use_managed_identity\") == \"True\"\n",
    "    backup_location = dbutils.widgets.get(\"backup_location\")\n",
    "    \n",
    "    # Check if this is external storage (not OneLake)\n",
    "    if use_managed_identity and \"dfs.core.windows.net\" in backup_location and \"onelake\" not in backup_location:\n",
    "        log_message(\"Configuring managed identity for external storage authentication\", \"INFO\")\n",
    "        \n",
    "        # Extract storage account name from path\n",
    "        import re\n",
    "        match = re.search(r'([^/@]+)\\.dfs\\.core\\.windows\\.net', backup_location)\n",
    "        if match:\n",
    "            account_name = match.group(1)\n",
    "            spark.conf.set(f\"fs.azure.account.auth.type.{account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "            spark.conf.set(f\"fs.azure.account.oauth.provider.type.{account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider\")\n",
    "            log_message(f\"Configured authentication for storage account: {account_name}\", \"INFO\")\n",
    "\n",
    "def parse_backup_folder_name(folder_name):\n",
    "    \"\"\"Parse backup folder name to extract timestamp and metadata\"\"\"\n",
    "    try:\n",
    "        # Expected format: backup_YYYY-MM-DD_HH-MM-SS_uuid\n",
    "        if not folder_name.startswith(\"backup_\"):\n",
    "            return None\n",
    "        \n",
    "        parts = folder_name.split(\"_\")\n",
    "        if len(parts) < 4:  # backup, date, time, uuid (minimum)\n",
    "            return None\n",
    "        \n",
    "        date_part = parts[1]  # YYYY-MM-DD\n",
    "        time_part = parts[2]  # HH-MM-SS\n",
    "        backup_id = parts[3] if len(parts) > 3 else \"unknown\"\n",
    "        \n",
    "        timestamp_str = f\"{date_part} {time_part.replace('-', ':')}\"\n",
    "        backup_timestamp = datetime.datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        return {\n",
    "            \"folder_name\": folder_name,\n",
    "            \"timestamp\": backup_timestamp,\n",
    "            \"backup_id\": backup_id,\n",
    "            \"date_part\": date_part,\n",
    "            \"time_part\": time_part\n",
    "        }\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error parsing backup folder name '{folder_name}': {str(e)}\", \"WARNING\")\n",
    "        return None\n",
    "\n",
    "def get_backup_size(backup_path):\n",
    "    \"\"\"Calculate total size of a backup in bytes\"\"\"\n",
    "    try:\n",
    "        total_size = 0\n",
    "        files = dbutils.fs.ls(backup_path)\n",
    "        \n",
    "        for file_info in files:\n",
    "            if file_info.isDir:\n",
    "                # Recursively calculate directory size\n",
    "                total_size += get_backup_size(file_info.path)\n",
    "            else:\n",
    "                total_size += file_info.size\n",
    "        \n",
    "        return total_size\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error calculating backup size for {backup_path}: {str(e)}\", \"WARNING\")\n",
    "        return 0\n",
    "\n",
    "def read_backup_manifest(backup_path):\n",
    "    \"\"\"Read backup manifest if available\"\"\"\n",
    "    try:\n",
    "        manifest_path = f\"{backup_path}/_manifest\"\n",
    "        manifest_df = spark.read.format(\"json\").load(manifest_path)\n",
    "        manifest_data = manifest_df.collect()[0].asDict()\n",
    "        return manifest_data\n",
    "    except Exception as e:\n",
    "        # Manifest not available or corrupted\n",
    "        return None\n",
    "\n",
    "def matches_pattern(text, patterns):\n",
    "    \"\"\"Check if text matches any of the given patterns (supports wildcards)\"\"\"\n",
    "    if not patterns:\n",
    "        return False\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        # Convert wildcard pattern to regex\n",
    "        regex_pattern = pattern.replace(\"*\", \".*\").replace(\"?\", \".\")\n",
    "        if re.match(f\"^{regex_pattern}$\", text):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "print(\"Cleanup helper functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9963b008",
   "metadata": {},
   "source": [
    "## 4. Backup Discovery and Analysis Functions\n",
    "\n",
    "Implement functions to discover backups, filter by include/exclude patterns, parse metadata, and apply preservation policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a522f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_backups(backup_location):\n",
    "    \"\"\"Discover all backups in the specified location\"\"\"\n",
    "    log_message(\"Discovering backups...\", \"INFO\")\n",
    "    \n",
    "    try:\n",
    "        backup_folders = dbutils.fs.ls(backup_location)\n",
    "        include_patterns = [p.strip() for p in dbutils.widgets.get(\"include_patterns\").split(\",\") if p.strip()]\n",
    "        exclude_patterns = [p.strip() for p in dbutils.widgets.get(\"exclude_patterns\").split(\",\") if p.strip()]\n",
    "        \n",
    "        backups = []\n",
    "        \n",
    "        for folder in backup_folders:\n",
    "            if not folder.isDir:\n",
    "                continue\n",
    "            \n",
    "            folder_name = folder.name.rstrip('/')\n",
    "            \n",
    "            # Check include patterns\n",
    "            if include_patterns and not matches_pattern(folder_name, include_patterns):\n",
    "                continue\n",
    "            \n",
    "            # Check exclude patterns\n",
    "            if exclude_patterns and matches_pattern(folder_name, exclude_patterns):\n",
    "                log_message(f\"Excluding backup {folder_name} (matches exclude pattern)\", \"INFO\")\n",
    "                continue\n",
    "            \n",
    "            # Parse backup folder\n",
    "            backup_info = parse_backup_folder_name(folder_name)\n",
    "            if not backup_info:\n",
    "                log_message(f\"Skipping folder {folder_name} (invalid backup name format)\", \"INFO\")\n",
    "                continue\n",
    "            \n",
    "            # Get backup size\n",
    "            backup_size = get_backup_size(folder.path)\n",
    "            \n",
    "            # Read manifest if available\n",
    "            manifest_data = read_backup_manifest(folder.path)\n",
    "            \n",
    "            # Compile backup information\n",
    "            backup_record = {\n",
    "                \"folder_name\": folder_name,\n",
    "                \"full_path\": folder.path,\n",
    "                \"timestamp\": backup_info[\"timestamp\"],\n",
    "                \"backup_id\": backup_info[\"backup_id\"],\n",
    "                \"size_bytes\": backup_size,\n",
    "                \"size_mb\": backup_size / (1024 * 1024),\n",
    "                \"size_gb\": backup_size / (1024 * 1024 * 1024),\n",
    "                \"age_days\": (datetime.datetime.now() - backup_info[\"timestamp\"]).days,\n",
    "                \"manifest\": manifest_data\n",
    "            }\n",
    "            \n",
    "            # Add manifest information if available\n",
    "            if manifest_data:\n",
    "                backup_record.update({\n",
    "                    \"source_lakehouse\": manifest_data.get(\"source_lakehouse_name\", \"Unknown\"),\n",
    "                    \"tables_count\": len(manifest_data.get(\"tables\", [])),\n",
    "                    \"backup_type\": manifest_data.get(\"backup_type\", \"Unknown\")\n",
    "                })\n",
    "            else:\n",
    "                backup_record.update({\n",
    "                    \"source_lakehouse\": \"Unknown\",\n",
    "                    \"tables_count\": 0,\n",
    "                    \"backup_type\": \"Unknown\"\n",
    "                })\n",
    "            \n",
    "            backups.append(backup_record)\n",
    "        \n",
    "        # Sort backups by timestamp (newest first)\n",
    "        backups.sort(key=lambda x: x[\"timestamp\"], reverse=True)\n",
    "        \n",
    "        log_message(f\"Discovered {len(backups)} backups\", \"INFO\")\n",
    "        return backups\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error discovering backups: {str(e)}\", \"ERROR\")\n",
    "        return []\n",
    "\n",
    "def apply_preservation_policy(backups):\n",
    "    \"\"\"Apply preservation policy to protect certain backups\"\"\"\n",
    "    preserve_policy = dbutils.widgets.get(\"preserve_policy\")\n",
    "    \n",
    "    if preserve_policy == \"none\":\n",
    "        return set()  # No backups to preserve\n",
    "    \n",
    "    preserved = set()\n",
    "    \n",
    "    try:\n",
    "        preserve_daily_count = int(dbutils.widgets.get(\"preserve_daily_count\"))\n",
    "        preserve_weekly_count = int(dbutils.widgets.get(\"preserve_weekly_count\"))\n",
    "        preserve_monthly_count = int(dbutils.widgets.get(\"preserve_monthly_count\"))\n",
    "    except ValueError:\n",
    "        log_message(\"Invalid preservation counts, using defaults\", \"WARNING\")\n",
    "        preserve_daily_count = 7\n",
    "        preserve_weekly_count = 4\n",
    "        preserve_monthly_count = 12\n",
    "    \n",
    "    # Group backups by date patterns\n",
    "    daily_backups = defaultdict(list)\n",
    "    weekly_backups = defaultdict(list)\n",
    "    monthly_backups = defaultdict(list)\n",
    "    \n",
    "    for backup in backups:\n",
    "        timestamp = backup[\"timestamp\"]\n",
    "        date_key = timestamp.strftime(\"%Y-%m-%d\")\n",
    "        week_key = timestamp.strftime(\"%Y-W%U\")  # Year-Week\n",
    "        month_key = timestamp.strftime(\"%Y-%m\")   # Year-Month\n",
    "        \n",
    "        daily_backups[date_key].append(backup)\n",
    "        weekly_backups[week_key].append(backup)\n",
    "        monthly_backups[month_key].append(backup)\n",
    "    \n",
    "    # Preserve daily backups (most recent per day)\n",
    "    if preserve_policy in [\"daily\", \"weekly\", \"monthly\"]:\n",
    "        daily_dates = sorted(daily_backups.keys(), reverse=True)[:preserve_daily_count]\n",
    "        for date_key in daily_dates:\n",
    "            # Keep the most recent backup for each day\n",
    "            newest_daily = max(daily_backups[date_key], key=lambda x: x[\"timestamp\"])\n",
    "            preserved.add(newest_daily[\"folder_name\"])\n",
    "            log_message(f\"Preserving daily backup: {newest_daily['folder_name']}\", \"INFO\")\n",
    "    \n",
    "    # Preserve weekly backups (most recent per week)\n",
    "    if preserve_policy in [\"weekly\", \"monthly\"]:\n",
    "        weekly_keys = sorted(weekly_backups.keys(), reverse=True)[:preserve_weekly_count]\n",
    "        for week_key in weekly_keys:\n",
    "            # Keep the most recent backup for each week\n",
    "            newest_weekly = max(weekly_backups[week_key], key=lambda x: x[\"timestamp\"])\n",
    "            preserved.add(newest_weekly[\"folder_name\"])\n",
    "            log_message(f\"Preserving weekly backup: {newest_weekly['folder_name']}\", \"INFO\")\n",
    "    \n",
    "    # Preserve monthly backups (most recent per month)\n",
    "    if preserve_policy == \"monthly\":\n",
    "        monthly_keys = sorted(monthly_backups.keys(), reverse=True)[:preserve_monthly_count]\n",
    "        for month_key in monthly_keys:\n",
    "            # Keep the most recent backup for each month\n",
    "            newest_monthly = max(monthly_backups[month_key], key=lambda x: x[\"timestamp\"])\n",
    "            preserved.add(newest_monthly[\"folder_name\"])\n",
    "            log_message(f\"Preserving monthly backup: {newest_monthly['folder_name']}\", \"INFO\")\n",
    "    \n",
    "    log_message(f\"Preservation policy '{preserve_policy}' protects {len(preserved)} backups\", \"INFO\")\n",
    "    return preserved\n",
    "\n",
    "def determine_backups_to_delete(backups):\n",
    "    \"\"\"Determine which backups should be deleted based on cleanup mode\"\"\"\n",
    "    cleanup_mode = dbutils.widgets.get(\"cleanup_mode\")\n",
    "    preserved_backups = apply_preservation_policy(backups)\n",
    "    \n",
    "    backups_to_delete = []\n",
    "    \n",
    "    if cleanup_mode == \"age_based\":\n",
    "        retention_days = int(dbutils.widgets.get(\"retention_days\"))\n",
    "        log_message(f\"Age-based cleanup: removing backups older than {retention_days} days\", \"INFO\")\n",
    "        \n",
    "        for backup in backups:\n",
    "            if backup[\"folder_name\"] not in preserved_backups and backup[\"age_days\"] > retention_days:\n",
    "                backups_to_delete.append(backup)\n",
    "    \n",
    "    elif cleanup_mode == \"count_based\":\n",
    "        max_count = int(dbutils.widgets.get(\"max_backup_count\"))\n",
    "        log_message(f\"Count-based cleanup: keeping only {max_count} most recent backups\", \"INFO\")\n",
    "        \n",
    "        # Filter out preserved backups first\n",
    "        non_preserved = [b for b in backups if b[\"folder_name\"] not in preserved_backups]\n",
    "        \n",
    "        # Sort by timestamp (newest first) and keep only max_count\n",
    "        non_preserved.sort(key=lambda x: x[\"timestamp\"], reverse=True)\n",
    "        \n",
    "        if len(non_preserved) > max_count:\n",
    "            backups_to_delete = non_preserved[max_count:]\n",
    "    \n",
    "    elif cleanup_mode == \"size_based\":\n",
    "        max_size_gb = float(dbutils.widgets.get(\"max_size_gb\"))\n",
    "        max_size_bytes = max_size_gb * 1024 * 1024 * 1024\n",
    "        \n",
    "        log_message(f\"Size-based cleanup: keeping total size under {max_size_gb:.2f} GB\", \"INFO\")\n",
    "        \n",
    "        # Calculate current total size\n",
    "        total_size = sum(b[\"size_bytes\"] for b in backups)\n",
    "        log_message(f\"Current total size: {total_size / (1024*1024*1024):.2f} GB\", \"INFO\")\n",
    "        \n",
    "        if total_size > max_size_bytes:\n",
    "            # Sort by timestamp (oldest first) and remove until under limit\n",
    "            non_preserved = [b for b in backups if b[\"folder_name\"] not in preserved_backups]\n",
    "            non_preserved.sort(key=lambda x: x[\"timestamp\"])  # Oldest first\n",
    "            \n",
    "            current_size = total_size\n",
    "            for backup in non_preserved:\n",
    "                if current_size <= max_size_bytes:\n",
    "                    break\n",
    "                backups_to_delete.append(backup)\n",
    "                current_size -= backup[\"size_bytes\"]\n",
    "    \n",
    "    elif cleanup_mode == \"advanced\":\n",
    "        # Advanced mode combines all criteria\n",
    "        log_message(\"Advanced cleanup mode: applying multiple criteria\", \"INFO\")\n",
    "        \n",
    "        retention_days = int(dbutils.widgets.get(\"retention_days\"))\n",
    "        max_count = int(dbutils.widgets.get(\"max_backup_count\"))\n",
    "        max_size_gb = float(dbutils.widgets.get(\"max_size_gb\"))\n",
    "        max_size_bytes = max_size_gb * 1024 * 1024 * 1024\n",
    "        \n",
    "        # First, apply age-based deletion\n",
    "        age_candidates = [b for b in backups \n",
    "                         if b[\"folder_name\"] not in preserved_backups and b[\"age_days\"] > retention_days]\n",
    "        \n",
    "        # Then apply count-based to remaining\n",
    "        remaining = [b for b in backups if b not in age_candidates and b[\"folder_name\"] not in preserved_backups]\n",
    "        remaining.sort(key=lambda x: x[\"timestamp\"], reverse=True)\n",
    "        \n",
    "        count_candidates = remaining[max_count:] if len(remaining) > max_count else []\n",
    "        \n",
    "        # Combine age and count candidates\n",
    "        preliminary_deletions = age_candidates + count_candidates\n",
    "        \n",
    "        # Finally, apply size-based if needed\n",
    "        remaining_after_deletions = [b for b in backups if b not in preliminary_deletions]\n",
    "        remaining_size = sum(b[\"size_bytes\"] for b in remaining_after_deletions)\n",
    "        \n",
    "        if remaining_size > max_size_bytes:\n",
    "            # Need to delete more based on size\n",
    "            additional_candidates = [b for b in remaining_after_deletions \n",
    "                                   if b[\"folder_name\"] not in preserved_backups]\n",
    "            additional_candidates.sort(key=lambda x: x[\"timestamp\"])  # Oldest first\n",
    "            \n",
    "            current_size = remaining_size\n",
    "            for backup in additional_candidates:\n",
    "                if current_size <= max_size_bytes:\n",
    "                    break\n",
    "                preliminary_deletions.append(backup)\n",
    "                current_size -= backup[\"size_bytes\"]\n",
    "        \n",
    "        backups_to_delete = preliminary_deletions\n",
    "    \n",
    "    log_message(f\"Determined {len(backups_to_delete)} backups for deletion\", \"INFO\")\n",
    "    return backups_to_delete\n",
    "\n",
    "print(\"Backup analysis functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca0ba0",
   "metadata": {},
   "source": [
    "## 5. Cleanup Execution and Reporting Functions\n",
    "\n",
    "Provide functions to determine which backups to delete, execute deletions (with dry run support), and generate cleanup reports with summary and details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a0ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_cleanup(backups_to_delete):\n",
    "    \"\"\"Execute the cleanup operation\"\"\"\n",
    "    dry_run = dbutils.widgets.get(\"dry_run\") == \"True\"\n",
    "    \n",
    "    if dry_run:\n",
    "        log_message(\"DRY RUN MODE: No backups will be actually deleted\", \"INFO\")\n",
    "    else:\n",
    "        log_message(\"LIVE MODE: Backups will be permanently deleted\", \"WARNING\")\n",
    "    \n",
    "    deletion_results = []\n",
    "    total_size_freed = 0\n",
    "    successful_deletions = 0\n",
    "    failed_deletions = 0\n",
    "    \n",
    "    for backup in backups_to_delete:\n",
    "        deletion_record = {\n",
    "            \"folder_name\": backup[\"folder_name\"],\n",
    "            \"full_path\": backup[\"full_path\"],\n",
    "            \"size_bytes\": backup[\"size_bytes\"],\n",
    "            \"age_days\": backup[\"age_days\"],\n",
    "            \"timestamp\": backup[\"timestamp\"],\n",
    "            \"deleted\": False,\n",
    "            \"error\": None\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            log_message(f\"{'[DRY RUN] Would delete' if dry_run else 'Deleting'} backup: {backup['folder_name']} \"\n",
    "                       f\"(Age: {backup['age_days']} days, Size: {backup['size_mb']:.2f} MB)\", \"INFO\")\n",
    "            \n",
    "            if not dry_run:\n",
    "                # Actually delete the backup\n",
    "                dbutils.fs.rm(backup[\"full_path\"], True)\n",
    "                deletion_record[\"deleted\"] = True\n",
    "                successful_deletions += 1\n",
    "                total_size_freed += backup[\"size_bytes\"]\n",
    "                log_message(f\"Successfully deleted backup: {backup['folder_name']}\", \"INFO\")\n",
    "            else:\n",
    "                # Dry run - just simulate\n",
    "                deletion_record[\"deleted\"] = True  # Would be deleted\n",
    "                successful_deletions += 1\n",
    "                total_size_freed += backup[\"size_bytes\"]\n",
    "        \n",
    "        except Exception as e:\n",
    "            deletion_record[\"error\"] = str(e)\n",
    "            failed_deletions += 1\n",
    "            log_message(f\"Error {'simulating deletion of' if dry_run else 'deleting'} backup {backup['folder_name']}: {str(e)}\", \"ERROR\")\n",
    "        \n",
    "        deletion_results.append(deletion_record)\n",
    "    \n",
    "    # Summary\n",
    "    log_message(\"\", \"INFO\")\n",
    "    log_message(\"=== CLEANUP SUMMARY ===\", \"INFO\")\n",
    "    log_message(f\"Mode: {'DRY RUN' if dry_run else 'LIVE EXECUTION'}\", \"INFO\")\n",
    "    log_message(f\"Backups processed: {len(backups_to_delete)}\", \"INFO\")\n",
    "    log_message(f\"Successful {'simulations' if dry_run else 'deletions'}: {successful_deletions}\", \"INFO\")\n",
    "    log_message(f\"Failed operations: {failed_deletions}\", \"INFO\")\n",
    "    log_message(f\"Space {'that would be' if dry_run else ''} freed: {total_size_freed / (1024*1024*1024):.2f} GB\", \"INFO\")\n",
    "    log_message(\"=======================\", \"INFO\")\n",
    "    \n",
    "    return {\n",
    "        \"dry_run\": dry_run,\n",
    "        \"total_processed\": len(backups_to_delete),\n",
    "        \"successful_operations\": successful_deletions,\n",
    "        \"failed_operations\": failed_deletions,\n",
    "        \"space_freed_bytes\": total_size_freed,\n",
    "        \"space_freed_gb\": total_size_freed / (1024*1024*1024),\n",
    "        \"deletion_results\": deletion_results\n",
    "    }\n",
    "\n",
    "def generate_cleanup_report(backups_discovered, backups_preserved, backups_deleted, cleanup_results):\n",
    "    \"\"\"Generate a comprehensive cleanup report\"\"\"\n",
    "    generate_report = dbutils.widgets.get(\"generate_report\") == \"True\"\n",
    "    \n",
    "    if not generate_report:\n",
    "        return None\n",
    "    \n",
    "    log_message(\"Generating cleanup report...\", \"INFO\")\n",
    "    \n",
    "    # Create report data\n",
    "    report_data = {\n",
    "        \"report_timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"cleanup_mode\": dbutils.widgets.get(\"cleanup_mode\"),\n",
    "        \"backup_location\": dbutils.widgets.get(\"backup_location\"),\n",
    "        \"dry_run\": dbutils.widgets.get(\"dry_run\") == \"True\",\n",
    "        \"preserve_policy\": dbutils.widgets.get(\"preserve_policy\"),\n",
    "        \"total_backups_discovered\": len(backups_discovered),\n",
    "        \"total_backups_preserved\": len(backups_preserved),\n",
    "        \"total_backups_deleted\": cleanup_results[\"successful_operations\"],\n",
    "        \"total_size_discovered_gb\": sum(b[\"size_gb\"] for b in backups_discovered),\n",
    "        \"total_size_freed_gb\": cleanup_results[\"space_freed_gb\"],\n",
    "        \"oldest_backup\": min(backups_discovered, key=lambda x: x[\"timestamp\"])[\"timestamp\"].strftime(\"%Y-%m-%d %H:%M:%S\") if backups_discovered else \"N/A\",\n",
    "        \"newest_backup\": max(backups_discovered, key=lambda x: x[\"timestamp\"])[\"timestamp\"].strftime(\"%Y-%m-%d %H:%M:%S\") if backups_discovered else \"N/A\"\n",
    "    }\n",
    "    \n",
    "    # Create detailed tables for reporting\n",
    "    try:\n",
    "        # Discovered backups summary\n",
    "        discovered_summary = []\n",
    "        for backup in backups_discovered:\n",
    "            discovered_summary.append({\n",
    "                \"folder_name\": backup[\"folder_name\"],\n",
    "                \"timestamp\": backup[\"timestamp\"].strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"age_days\": backup[\"age_days\"],\n",
    "                \"size_gb\": backup[\"size_gb\"],\n",
    "                \"source_lakehouse\": backup[\"source_lakehouse\"],\n",
    "                \"tables_count\": backup[\"tables_count\"],\n",
    "                \"status\": \"Preserved\" if backup[\"folder_name\"] in backups_preserved else \"Available for cleanup\"\n",
    "            })\n",
    "        \n",
    "        # Deletion details\n",
    "        deletion_summary = cleanup_results[\"deletion_results\"]\n",
    "        \n",
    "        # Save report as Delta tables in the backup location (if possible)\n",
    "        try:\n",
    "            backup_location = dbutils.widgets.get(\"backup_location\")\n",
    "            report_base_path = f\"{backup_location}/_cleanup_reports\"\n",
    "            \n",
    "            # Create report folder\n",
    "            dbutils.fs.mkdirs(report_base_path)\n",
    "            \n",
    "            # Save report metadata\n",
    "            report_df = spark.createDataFrame([report_data])\n",
    "            report_timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            report_df.write.mode(\"overwrite\").format(\"json\").save(f\"{report_base_path}/cleanup_report_{report_timestamp}\")\n",
    "            \n",
    "            # Save discovered backups summary\n",
    "            if discovered_summary:\n",
    "                discovered_df = spark.createDataFrame(discovered_summary)\n",
    "                discovered_df.write.mode(\"overwrite\").format(\"delta\").save(f\"{report_base_path}/discovered_backups_{report_timestamp}\")\n",
    "            \n",
    "            # Save deletion results\n",
    "            if deletion_summary:\n",
    "                deletion_df = spark.createDataFrame(deletion_summary)\n",
    "                deletion_df.write.mode(\"overwrite\").format(\"delta\").save(f\"{report_base_path}/deletion_results_{report_timestamp}\")\n",
    "            \n",
    "            log_message(f\"Cleanup report saved to: {report_base_path}/cleanup_report_{report_timestamp}\", \"INFO\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            log_message(f\"Could not save report to backup location: {str(e)}\", \"WARNING\")\n",
    "        \n",
    "        return report_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error generating cleanup report: {str(e)}\", \"ERROR\")\n",
    "        return None\n",
    "\n",
    "print(\"Cleanup execution and reporting functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d90540",
   "metadata": {},
   "source": [
    "## 6. Main Cleanup Execution\n",
    "\n",
    "Orchestrate the cleanup process: validate parameters, authenticate, discover backups, apply policies, determine deletions, execute cleanup, and summarize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd6e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_cleanup_operation():\n",
    "    \"\"\"Execute the main cleanup operation\"\"\"\n",
    "    \n",
    "    start_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_message(\"Starting Microsoft Fabric lakehouse backup cleanup process\", \"INFO\")\n",
    "    \n",
    "    try:\n",
    "        # Validate parameters\n",
    "        validate_cleanup_parameters()\n",
    "        \n",
    "        # Setup external storage authentication if needed\n",
    "        setup_external_storage_auth()\n",
    "        \n",
    "        # Get configuration values\n",
    "        backup_location = dbutils.widgets.get(\"backup_location\")\n",
    "        cleanup_mode = dbutils.widgets.get(\"cleanup_mode\")\n",
    "        dry_run = dbutils.widgets.get(\"dry_run\") == \"True\"\n",
    "        \n",
    "        log_message(f\"Cleanup location: {backup_location}\", \"INFO\")\n",
    "        log_message(f\"Cleanup mode: {cleanup_mode}\", \"INFO\")\n",
    "        log_message(f\"Dry run: {dry_run}\", \"INFO\")\n",
    "        \n",
    "        # Discover all backups\n",
    "        discovered_backups = discover_backups(backup_location)\n",
    "        \n",
    "        if not discovered_backups:\n",
    "            log_message(\"No backups found for cleanup\", \"WARNING\")\n",
    "            return {\n",
    "                \"status\": \"warning\",\n",
    "                \"message\": \"No backups found for cleanup\",\n",
    "                \"backups_processed\": 0\n",
    "            }\n",
    "        \n",
    "        # Apply preservation policy\n",
    "        preserved_backup_names = apply_preservation_policy(discovered_backups)\n",
    "        \n",
    "        # Determine which backups to delete\n",
    "        backups_to_delete = determine_backups_to_delete(discovered_backups)\n",
    "        \n",
    "        if not backups_to_delete:\n",
    "            log_message(\"No backups meet deletion criteria\", \"INFO\")\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"message\": \"No backups meet deletion criteria\",\n",
    "                \"backups_discovered\": len(discovered_backups),\n",
    "                \"backups_preserved\": len(preserved_backup_names),\n",
    "                \"backups_processed\": 0\n",
    "            }\n",
    "        \n",
    "        # Display what will be done\n",
    "        log_message(\"\", \"INFO\")\n",
    "        log_message(\"=== CLEANUP PLAN ===\", \"INFO\")\n",
    "        log_message(f\"Total backups discovered: {len(discovered_backups)}\", \"INFO\")\n",
    "        log_message(f\"Backups preserved by policy: {len(preserved_backup_names)}\", \"INFO\")\n",
    "        log_message(f\"Backups {'to be deleted' if not dry_run else 'that would be deleted'}: {len(backups_to_delete)}\", \"INFO\")\n",
    "        \n",
    "        total_size_to_free = sum(b[\"size_bytes\"] for b in backups_to_delete)\n",
    "        log_message(f\"Total space {'to be freed' if not dry_run else 'that would be freed'}: {total_size_to_free / (1024*1024*1024):.2f} GB\", \"INFO\")\n",
    "        log_message(\"====================\", \"INFO\")\n",
    "        \n",
    "        # Execute cleanup\n",
    "        cleanup_results = execute_cleanup(backups_to_delete)\n",
    "        \n",
    "        # Generate report\n",
    "        report_data = generate_cleanup_report(\n",
    "            discovered_backups, \n",
    "            preserved_backup_names, \n",
    "            backups_to_delete, \n",
    "            cleanup_results\n",
    "        )\n",
    "        \n",
    "        # Record end time\n",
    "        end_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        duration_seconds = (datetime.datetime.strptime(end_time, \"%Y-%m-%d %H:%M:%S\") - \n",
    "                           datetime.datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")).total_seconds()\n",
    "        \n",
    "        # Final summary\n",
    "        log_message(\"\", \"INFO\")\n",
    "        log_message(\"=== FINAL CLEANUP SUMMARY ===\", \"INFO\")\n",
    "        log_message(f\"Cleanup completed at: {end_time}\", \"INFO\")\n",
    "        log_message(f\"Duration: {duration_seconds:.2f} seconds\", \"INFO\")\n",
    "        log_message(f\"Mode: {'DRY RUN' if dry_run else 'LIVE EXECUTION'}\", \"INFO\")\n",
    "        log_message(f\"Backups discovered: {len(discovered_backups)}\", \"INFO\")\n",
    "        log_message(f\"Backups preserved: {len(preserved_backup_names)}\", \"INFO\")\n",
    "        log_message(f\"Backups {'simulated for deletion' if dry_run else 'deleted'}: {cleanup_results['successful_operations']}\", \"INFO\")\n",
    "        log_message(f\"Failed operations: {cleanup_results['failed_operations']}\", \"INFO\")\n",
    "        log_message(f\"Space {'that would be' if dry_run else ''} freed: {cleanup_results['space_freed_gb']:.2f} GB\", \"INFO\")\n",
    "        log_message(\"=============================\", \"INFO\")\n",
    "        \n",
    "        # Return comprehensive results\n",
    "        result = {\n",
    "            \"status\": \"success\" if cleanup_results[\"failed_operations\"] == 0 else \"partial_success\",\n",
    "            \"cleanup_mode\": cleanup_mode,\n",
    "            \"backup_location\": backup_location,\n",
    "            \"dry_run\": dry_run,\n",
    "            \"backups_discovered\": len(discovered_backups),\n",
    "            \"backups_preserved\": len(preserved_backup_names),\n",
    "            \"backups_processed\": cleanup_results[\"total_processed\"],\n",
    "            \"successful_operations\": cleanup_results[\"successful_operations\"],\n",
    "            \"failed_operations\": cleanup_results[\"failed_operations\"],\n",
    "            \"space_freed_gb\": cleanup_results[\"space_freed_gb\"],\n",
    "            \"duration_seconds\": duration_seconds,\n",
    "            \"report_data\": report_data\n",
    "        }\n",
    "        \n",
    "        if cleanup_results[\"failed_operations\"] == 0:\n",
    "            log_message(\"Cleanup operation completed successfully\", \"INFO\")\n",
    "        else:\n",
    "            log_message(f\"Cleanup operation completed with {cleanup_results['failed_operations']} failures\", \"WARNING\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Cleanup operation failed with error: {str(e)}\", \"ERROR\")\n",
    "        import traceback\n",
    "        log_message(f\"Full error trace: {traceback.format_exc()}\", \"ERROR\")\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "\n",
    "print(\"Main cleanup function defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21d127e",
   "metadata": {},
   "source": [
    "## 7. Execute Cleanup Operation\n",
    "\n",
    "Run the main cleanup operation, display configuration and results, and exit with a status for pipeline integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec497329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the cleanup operation\n",
    "print(\"Starting cleanup operation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display current configuration for review\n",
    "print(\"CURRENT CONFIGURATION:\")\n",
    "print(f\"  Cleanup Mode: {dbutils.widgets.get('cleanup_mode')}\")\n",
    "print(f\"  Backup Location: {dbutils.widgets.get('backup_location')}\")\n",
    "print(f\"  Dry Run: {dbutils.widgets.get('dry_run')}\")\n",
    "print(f\"  Preserve Policy: {dbutils.widgets.get('preserve_policy')}\")\n",
    "\n",
    "if dbutils.widgets.get(\"cleanup_mode\") == \"age_based\":\n",
    "    print(f\"  Retention Days: {dbutils.widgets.get('retention_days')}\")\n",
    "elif dbutils.widgets.get(\"cleanup_mode\") == \"count_based\":\n",
    "    print(f\"  Max Backup Count: {dbutils.widgets.get('max_backup_count')}\")\n",
    "elif dbutils.widgets.get(\"cleanup_mode\") == \"size_based\":\n",
    "    print(f\"  Max Size (GB): {dbutils.widgets.get('max_size_gb')}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Safety check for dry run\n",
    "dry_run_mode = dbutils.widgets.get(\"dry_run\") == \"True\"\n",
    "if not dry_run_mode:\n",
    "    print(\"âš ï¸  WARNING: DRY RUN IS DISABLED - BACKUPS WILL BE PERMANENTLY DELETED!\")\n",
    "    print(\"âš ï¸  Make sure you have reviewed the configuration above carefully.\")\n",
    "    print(\"âš ï¸  Consider running in dry_run mode first to preview the changes.\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "cleanup_result = execute_cleanup_operation()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Cleanup operation completed.\")\n",
    "print(f\"Final status: {cleanup_result['status']}\")\n",
    "\n",
    "# Display results in notebook output\n",
    "if cleanup_result['status'] == 'success':\n",
    "    if cleanup_result.get('dry_run', False):\n",
    "        print(f\"âœ… DRY RUN SUCCESS: Would delete {cleanup_result['successful_operations']} backups\")\n",
    "        print(f\"ðŸ’¾ Space that would be freed: {cleanup_result['space_freed_gb']:.2f} GB\")\n",
    "        print(\"ðŸ”„ Set dry_run=False to execute actual cleanup\")\n",
    "    else:\n",
    "        print(f\"âœ… CLEANUP SUCCESS: Deleted {cleanup_result['successful_operations']} backups\")\n",
    "        print(f\"ðŸ’¾ Space freed: {cleanup_result['space_freed_gb']:.2f} GB\")\n",
    "elif cleanup_result['status'] == 'partial_success':\n",
    "    print(f\"âš ï¸  PARTIAL SUCCESS: {cleanup_result['successful_operations']} successful, {cleanup_result['failed_operations']} failed\")\n",
    "    print(f\"ðŸ’¾ Space freed: {cleanup_result['space_freed_gb']:.2f} GB\")\n",
    "else:\n",
    "    print(f\"âŒ CLEANUP FAILED: {cleanup_result.get('error', 'Unknown error')}\")\n",
    "\n",
    "print(f\"ðŸ“Š Backups discovered: {cleanup_result.get('backups_discovered', 0)}\")\n",
    "print(f\"ðŸ›¡ï¸  Backups preserved: {cleanup_result.get('backups_preserved', 0)}\")\n",
    "\n",
    "# Exit with results for pipeline integration\n",
    "dbutils.notebook.exit(cleanup_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dffcfe",
   "metadata": {},
   "source": [
    "## 8. Additional Utility Functions (Optional)\n",
    "\n",
    "Include optional functions for advanced scenarios: analyze storage usage, preview cleanup impact, and plan emergency cleanup.\n",
    "\n",
    "These functions can be used for advanced cleanup scenarios. Uncomment and run separately as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6254ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_backup_storage_usage(backup_location):\n",
    "    \"\"\"\n",
    "    Analyze storage usage patterns across backups\n",
    "    Usage: Run this cell separately to get detailed storage analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        backups = discover_backups(backup_location)\n",
    "        \n",
    "        if not backups:\n",
    "            print(\"No backups found for analysis\")\n",
    "            return\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_size_gb = sum(b[\"size_gb\"] for b in backups)\n",
    "        avg_size_gb = total_size_gb / len(backups)\n",
    "        largest_backup = max(backups, key=lambda x: x[\"size_gb\"])\n",
    "        smallest_backup = min(backups, key=lambda x: x[\"size_gb\"])\n",
    "        oldest_backup = min(backups, key=lambda x: x[\"timestamp\"])\n",
    "        newest_backup = max(backups, key=lambda x: x[\"timestamp\"])\n",
    "        \n",
    "        # Group by source lakehouse\n",
    "        by_lakehouse = defaultdict(list)\n",
    "        for backup in backups:\n",
    "            by_lakehouse[backup[\"source_lakehouse\"]].append(backup)\n",
    "        \n",
    "        # Group by age ranges\n",
    "        age_ranges = {\n",
    "            \"0-7 days\": [],\n",
    "            \"8-30 days\": [],\n",
    "            \"31-90 days\": [],\n",
    "            \"91-365 days\": [],\n",
    "            \"365+ days\": []\n",
    "        }\n",
    "        \n",
    "        for backup in backups:\n",
    "            age = backup[\"age_days\"]\n",
    "            if age <= 7:\n",
    "                age_ranges[\"0-7 days\"].append(backup)\n",
    "            elif age <= 30:\n",
    "                age_ranges[\"8-30 days\"].append(backup)\n",
    "            elif age <= 90:\n",
    "                age_ranges[\"31-90 days\"].append(backup)\n",
    "            elif age <= 365:\n",
    "                age_ranges[\"91-365 days\"].append(backup)\n",
    "            else:\n",
    "                age_ranges[\"365+ days\"].append(backup)\n",
    "        \n",
    "        # Display analysis\n",
    "        print(\"=\" * 60)\n",
    "        print(\"BACKUP STORAGE ANALYSIS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total Backups: {len(backups)}\")\n",
    "        print(f\"Total Storage Used: {total_size_gb:.2f} GB\")\n",
    "        print(f\"Average Backup Size: {avg_size_gb:.2f} GB\")\n",
    "        print(f\"Largest Backup: {largest_backup['folder_name']} ({largest_backup['size_gb']:.2f} GB)\")\n",
    "        print(f\"Smallest Backup: {smallest_backup['folder_name']} ({smallest_backup['size_gb']:.2f} GB)\")\n",
    "        print(f\"Oldest Backup: {oldest_backup['folder_name']} ({oldest_backup['age_days']} days old)\")\n",
    "        print(f\"Newest Backup: {newest_backup['folder_name']} ({newest_backup['age_days']} days old)\")\n",
    "        \n",
    "        print(\"\\nBREAKDOWN BY SOURCE LAKEHOUSE:\")\n",
    "        for lakehouse, lakehouse_backups in by_lakehouse.items():\n",
    "            lakehouse_size = sum(b[\"size_gb\"] for b in lakehouse_backups)\n",
    "            print(f\"  {lakehouse}: {len(lakehouse_backups)} backups, {lakehouse_size:.2f} GB\")\n",
    "        \n",
    "        print(\"\\nBREAKDOWN BY AGE:\")\n",
    "        for age_range, range_backups in age_ranges.items():\n",
    "            if range_backups:\n",
    "                range_size = sum(b[\"size_gb\"] for b in range_backups)\n",
    "                print(f\"  {age_range}: {len(range_backups)} backups, {range_size:.2f} GB\")\n",
    "        \n",
    "        return {\n",
    "            \"total_backups\": len(backups),\n",
    "            \"total_size_gb\": total_size_gb,\n",
    "            \"avg_size_gb\": avg_size_gb,\n",
    "            \"by_lakehouse\": dict(by_lakehouse),\n",
    "            \"by_age_range\": dict(age_ranges)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing backup storage: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def preview_cleanup_impact(backup_location, cleanup_mode, **kwargs):\n",
    "    \"\"\"\n",
    "    Preview what would happen with different cleanup settings\n",
    "    Usage: Run this cell separately to test different cleanup scenarios\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"PREVIEWING CLEANUP IMPACT: {cleanup_mode.upper()} MODE\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Temporarily override parameters for preview\n",
    "        original_cleanup_mode = dbutils.widgets.get(\"cleanup_mode\")\n",
    "        original_dry_run = dbutils.widgets.get(\"dry_run\")\n",
    "        \n",
    "        # Set preview parameters\n",
    "        dbutils.widgets.removeAll()\n",
    "        dbutils.widgets.text(\"backup_location\", backup_location, \"Backup Location\")\n",
    "        dbutils.widgets.dropdown(\"cleanup_mode\", cleanup_mode, [\"age_based\", \"count_based\", \"size_based\", \"advanced\"], \"Cleanup Mode\")\n",
    "        dbutils.widgets.dropdown(\"dry_run\", \"True\", [\"True\", \"False\"], \"Dry Run\")\n",
    "        \n",
    "        # Set mode-specific parameters\n",
    "        for key, value in kwargs.items():\n",
    "            if key == \"retention_days\":\n",
    "                dbutils.widgets.text(\"retention_days\", str(value), \"Retention Days\")\n",
    "            elif key == \"max_backup_count\":\n",
    "                dbutils.widgets.text(\"max_backup_count\", str(value), \"Max Backup Count\")\n",
    "            elif key == \"max_size_gb\":\n",
    "                dbutils.widgets.text(\"max_size_gb\", str(value), \"Max Size GB\")\n",
    "        \n",
    "        # Set default values for other parameters\n",
    "        dbutils.widgets.dropdown(\"preserve_policy\", \"none\", [\"none\", \"daily\", \"weekly\", \"monthly\"], \"Preserve Policy\")\n",
    "        dbutils.widgets.text(\"preserve_daily_count\", \"7\", \"Daily Count\")\n",
    "        dbutils.widgets.text(\"preserve_weekly_count\", \"4\", \"Weekly Count\")\n",
    "        dbutils.widgets.text(\"preserve_monthly_count\", \"12\", \"Monthly Count\")\n",
    "        dbutils.widgets.text(\"exclude_patterns\", \"\", \"Exclude Patterns\")\n",
    "        dbutils.widgets.text(\"include_patterns\", \"backup_*\", \"Include Patterns\")\n",
    "        dbutils.widgets.dropdown(\"use_managed_identity\", \"True\", [\"True\", \"False\"], \"Use Managed Identity\")\n",
    "        dbutils.widgets.dropdown(\"enable_detailed_logging\", \"False\", [\"True\", \"False\"], \"Enable Logging\")\n",
    "        dbutils.widgets.dropdown(\"generate_report\", \"False\", [\"True\", \"False\"], \"Generate Report\")\n",
    "        \n",
    "        # Run preview\n",
    "        result = execute_cleanup_operation()\n",
    "        \n",
    "        print(f\"PREVIEW RESULTS:\")\n",
    "        print(f\"  Backups that would be deleted: {result.get('successful_operations', 0)}\")\n",
    "        print(f\"  Space that would be freed: {result.get('space_freed_gb', 0):.2f} GB\")\n",
    "        print(f\"  Backups that would be preserved: {result.get('backups_preserved', 0)}\")\n",
    "        \n",
    "        # Restore original parameters\n",
    "        dbutils.widgets.removeAll()\n",
    "        # Note: In a real scenario, you'd want to restore all original parameters here\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error previewing cleanup impact: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def emergency_cleanup(backup_location, free_gb_target):\n",
    "    \"\"\"\n",
    "    Emergency cleanup to free up a specific amount of space\n",
    "    Usage: Run this cell separately for emergency space recovery\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"EMERGENCY CLEANUP: Attempting to free {free_gb_target} GB\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        backups = discover_backups(backup_location)\n",
    "        if not backups:\n",
    "            print(\"No backups found\")\n",
    "            return\n",
    "        \n",
    "        # Sort by age (oldest first) for emergency deletion\n",
    "        backups.sort(key=lambda x: x[\"timestamp\"])\n",
    "        \n",
    "        target_bytes = free_gb_target * 1024 * 1024 * 1024\n",
    "        current_freed = 0\n",
    "        emergency_deletions = []\n",
    "        \n",
    "        for backup in backups:\n",
    "            if current_freed >= target_bytes:\n",
    "                break\n",
    "            emergency_deletions.append(backup)\n",
    "            current_freed += backup[\"size_bytes\"]\n",
    "        \n",
    "        print(f\"Emergency plan would delete {len(emergency_deletions)} backups\")\n",
    "        print(f\"Space that would be freed: {current_freed / (1024*1024*1024):.2f} GB\")\n",
    "        print(\"\\nBackups selected for emergency deletion:\")\n",
    "        \n",
    "        for backup in emergency_deletions:\n",
    "            print(f\"  - {backup['folder_name']} ({backup['age_days']} days, {backup['size_gb']:.2f} GB)\")\n",
    "        \n",
    "        print(\"\\nâš ï¸  This is a PREVIEW only. To execute:\")\n",
    "        print(\"1. Set cleanup_mode='age_based' with very low retention_days\")\n",
    "        print(\"2. Or manually delete specific backups\")\n",
    "        print(\"3. Or use size_based cleanup with lower max_size_gb\")\n",
    "        \n",
    "        return emergency_deletions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in emergency cleanup planning: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "# backup_location = \"abfss://backups@mystorageaccount.dfs.core.windows.net\"\n",
    "# \n",
    "# # Analyze current storage usage\n",
    "# storage_analysis = analyze_backup_storage_usage(backup_location)\n",
    "# \n",
    "# # Preview different cleanup scenarios\n",
    "# preview_age_30 = preview_cleanup_impact(backup_location, \"age_based\", retention_days=30)\n",
    "# preview_count_5 = preview_cleanup_impact(backup_location, \"count_based\", max_backup_count=5)\n",
    "# preview_size_50 = preview_cleanup_impact(backup_location, \"size_based\", max_size_gb=50)\n",
    "# \n",
    "# # Emergency cleanup planning\n",
    "# emergency_plan = emergency_cleanup(backup_location, free_gb_target=20)\n",
    "\n",
    "print(\"Advanced utility functions defined. Uncomment and modify the example lines above to use them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f5394",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup Notebook Execution Completed\n",
    "\n",
    "### ðŸ’¡ USAGE TIPS:\n",
    "1. Always test with `dry_run=True` first\n",
    "2. Use `preserve_policy` to protect important backups\n",
    "3. Schedule this notebook to run regularly for automated cleanup\n",
    "4. Monitor the cleanup reports for trends and optimization\n",
    "5. Use the utility functions in Section 8 for advanced analysis\n",
    "\n",
    "### ðŸ”— INTEGRATION:\n",
    "- Schedule via Fabric notebook scheduler\n",
    "- Add to Data Factory pipeline after backup jobs\n",
    "- Use REST API for programmatic execution\n",
    "- Combine with monitoring alerts for storage thresholds\n",
    "\n",
    "**End of Cleanup Notebook**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
